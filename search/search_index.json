{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RKE2, also known as RKE Government, is Rancher's next-generation Kubernetes distribution. It is a fully conformant ( currently under review ) Kubernetes distribution that focuses on security and compliance within the U.S. Federal Government sector. To meet these goals, RKE2 does the following: Provides defaults and configuration options that allow clusters to pass the CIS Kubernetes Benchmark with minimal operator intervention Enables FIPS 140-2 compliance Regularly scans components for CVEs using trivy in our build pipeline How is this different from RKE or K3s? \u00b6 RKE2 combines the best-of-both-worlds from the 1.x version of RKE (hereafter referred to as RKE1) and K3s. From K3s, it inherits the usability, ease-of-operations, and deployment model. From RKE1, it inherits close alignment with upstream Kubernetes. In places K3s has diverged from upstream Kubernetes in order to optimize for edge deployments, but RKE1 and RKE2 can stay closely aligned with upstream. Importantly, RKE2 does not rely on Docker as RKE1 does. RKE1 leveraged Docker for deploying and managing the control plane components as well as the container runtime for Kubernetes. RKE2 launches control plane components as static pods, managed by the kubelet. The embedded container runtime is containerd. Why two names? \u00b6 There are a few reasons why this distribution is known as both RKE2 and RKE Government. It is known as RKE Government in order to convey the primary use cases and sector it currently targets. It is known as RKE2 because it is the future of the RKE distribution. Right now, it is entirely independent from RKE1, but our next phase of development will focus on a seamless upgrade path and feature parity with RKE1 when integrated with the Rancher multi-cluster management platform. Once we've completed the upgrade path and Rancher-integration feature parity work, RKE1 and RKE Government will converge into a single distribution.","title":"Home"},{"location":"#how-is-this-different-from-rke-or-k3s","text":"RKE2 combines the best-of-both-worlds from the 1.x version of RKE (hereafter referred to as RKE1) and K3s. From K3s, it inherits the usability, ease-of-operations, and deployment model. From RKE1, it inherits close alignment with upstream Kubernetes. In places K3s has diverged from upstream Kubernetes in order to optimize for edge deployments, but RKE1 and RKE2 can stay closely aligned with upstream. Importantly, RKE2 does not rely on Docker as RKE1 does. RKE1 leveraged Docker for deploying and managing the control plane components as well as the container runtime for Kubernetes. RKE2 launches control plane components as static pods, managed by the kubelet. The embedded container runtime is containerd.","title":"How is this different from RKE or K3s?"},{"location":"#why-two-names","text":"There are a few reasons why this distribution is known as both RKE2 and RKE Government. It is known as RKE Government in order to convey the primary use cases and sector it currently targets. It is known as RKE2 because it is the future of the RKE distribution. Right now, it is entirely independent from RKE1, but our next phase of development will focus on a seamless upgrade path and feature parity with RKE1 when integrated with the Rancher multi-cluster management platform. Once we've completed the upgrade path and Rancher-integration feature parity work, RKE1 and RKE Government will converge into a single distribution.","title":"Why two names?"},{"location":"advanced/","text":"Advanced Options and Configuration \u00b6 This section contains advanced information describing the different ways you can run and manage RKE2: Certificate rotation Auto-deploying manifests Configuring containerd Secrets Encryption Config Node labels and taints Starting the server with the installation script Certificate Rotation \u00b6 By default, certificates in RKE2 expire in 12 months. If the certificates are expired or have fewer than 90 days remaining before they expire, the certificates are rotated when RKE2 is restarted. Auto-Deploying Manifests \u00b6 Any file found in /var/lib/rancher/rke2/server/manifests will automatically be deployed to Kubernetes in a manner similar to kubectl apply . For information about deploying Helm charts using the manifests directory, refer to the section about Helm. Configuring containerd \u00b6 RKE2 will generate config.toml for containerd in /var/lib/rancher/rke2/agent/etc/containerd/config.toml . For advanced customization for this file you can create another file called config.toml.tmpl in the same directory and it will be used instead. The config.toml.tmpl will be treated as a Go template file, and the config.Node structure is being passed to the template. See this template for an example of how to use the structure to customize the configuration file. Secrets Encryption Config \u00b6 RKE2 supports encrypting Secrets at rest, and will do the following automatically: Generate an AES-CBC key Generate an encryption config file with the generated key: { \"kind\" : \"EncryptionConfiguration\" , \"apiVersion\" : \"apiserver.config.k8s.io/v1\" , \"resources\" : [ { \"resources\" : [ \"secrets\" ], \"providers\" : [ { \"aescbc\" : { \"keys\" : [ { \"name\" : \"aescbckey\" , \"secret\" : \"xxxxxxxxxxxxxxxxxxx\" } ] } }, { \"identity\" : {} } ] } ] } Pass the config to the Kubernetes APIServer as encryption-provider-config Once enabled any created secret will be encrypted with this key. Note that if you disable encryption then any encrypted secrets will not be readable until you enable encryption again using the same key. Node Labels and Taints \u00b6 RKE2 agents can be configured with the options --node-label and --node-taint which adds a label and taint to the kubelet. The two options only add labels and/or taints at registration time [FIXME] , so they can only be added once and not changed after that again by running RKE2 commands. If you want to change node labels and taints after node registration you should use kubectl . Refer to the official Kubernetes documentation for details on how to add taints and node labels. Starting the Server with the Installation Script \u00b6 The installation script provides units for systemd, but does not enable or start the service by default. When running with systemd, logs will be created in /var/log/syslog and viewed using journalctl -u rke2-server or journalctl -u rke2-agent . An example of installing with the install script: curl -sfL https://get.rke2.io | sh - systemctl enable rke2-server systemctl start rke2-server","title":"Advanced Options and Configuration"},{"location":"advanced/#advanced-options-and-configuration","text":"This section contains advanced information describing the different ways you can run and manage RKE2: Certificate rotation Auto-deploying manifests Configuring containerd Secrets Encryption Config Node labels and taints Starting the server with the installation script","title":"Advanced Options and Configuration"},{"location":"advanced/#certificate-rotation","text":"By default, certificates in RKE2 expire in 12 months. If the certificates are expired or have fewer than 90 days remaining before they expire, the certificates are rotated when RKE2 is restarted.","title":"Certificate Rotation"},{"location":"advanced/#auto-deploying-manifests","text":"Any file found in /var/lib/rancher/rke2/server/manifests will automatically be deployed to Kubernetes in a manner similar to kubectl apply . For information about deploying Helm charts using the manifests directory, refer to the section about Helm.","title":"Auto-Deploying Manifests"},{"location":"advanced/#configuring-containerd","text":"RKE2 will generate config.toml for containerd in /var/lib/rancher/rke2/agent/etc/containerd/config.toml . For advanced customization for this file you can create another file called config.toml.tmpl in the same directory and it will be used instead. The config.toml.tmpl will be treated as a Go template file, and the config.Node structure is being passed to the template. See this template for an example of how to use the structure to customize the configuration file.","title":"Configuring containerd"},{"location":"advanced/#secrets-encryption-config","text":"RKE2 supports encrypting Secrets at rest, and will do the following automatically: Generate an AES-CBC key Generate an encryption config file with the generated key: { \"kind\" : \"EncryptionConfiguration\" , \"apiVersion\" : \"apiserver.config.k8s.io/v1\" , \"resources\" : [ { \"resources\" : [ \"secrets\" ], \"providers\" : [ { \"aescbc\" : { \"keys\" : [ { \"name\" : \"aescbckey\" , \"secret\" : \"xxxxxxxxxxxxxxxxxxx\" } ] } }, { \"identity\" : {} } ] } ] } Pass the config to the Kubernetes APIServer as encryption-provider-config Once enabled any created secret will be encrypted with this key. Note that if you disable encryption then any encrypted secrets will not be readable until you enable encryption again using the same key.","title":"Secrets Encryption Config"},{"location":"advanced/#node-labels-and-taints","text":"RKE2 agents can be configured with the options --node-label and --node-taint which adds a label and taint to the kubelet. The two options only add labels and/or taints at registration time [FIXME] , so they can only be added once and not changed after that again by running RKE2 commands. If you want to change node labels and taints after node registration you should use kubectl . Refer to the official Kubernetes documentation for details on how to add taints and node labels.","title":"Node Labels and Taints"},{"location":"advanced/#starting-the-server-with-the-installation-script","text":"The installation script provides units for systemd, but does not enable or start the service by default. When running with systemd, logs will be created in /var/log/syslog and viewed using journalctl -u rke2-server or journalctl -u rke2-agent . An example of installing with the install script: curl -sfL https://get.rke2.io | sh - systemctl enable rke2-server systemctl start rke2-server","title":"Starting the Server with the Installation Script"},{"location":"backup_restore/","text":"Etcd Backup and Restore \u00b6 In this section, you'll learn how to create backups of the rke2 cluster data and to restore the cluster from backup. Note: /var/lib/rancher/rke2 is the default data directory for rke2, its however configurable via --data-dir Creating Snapshots \u00b6 Snapshots are enabled by default. The snapshot directory defaults to /var/lib/rancher/rke2/server/db/snapshots . To configure the snapshot interval or the number of retained snapshots, refer to the options. Cluster Reset \u00b6 RKE2 enables a feature to reset the cluster to one member cluster by passing --cluster-reset flag, when passing this flag to rke2 server it will reset the cluster with the same data dir in place, the data directory for etcd exists in /var/lib/rancher/rke2/server/db/etcd , this flag can be passed in the events of quorum loss in the cluster. To pass the reset flag, first you need to stop RKE2 service if its enabled via systemd: systemctl stop rke2-server rke2 server --cluster-reset Result: A message in the logs say that RKE2 can be restarted without the flags. Start rke2 again and it should start rke2 as a 1 member cluster. Restoring a Cluster from a Snapshot \u00b6 When RKE2 is restored from backup, the old data directory will be moved to /var/lib/rancher/rke2/server/db/etcd-old-%date%/ . Then RKE2 will attempt to restore the snapshot by creating a new data directory, then starting etcd with a new RKE2 cluster with one etcd member. To restore the cluster from backup, run RKE2 with the --cluster-reset option, with the --cluster-reset-restore-path also given: rke2 server \\ --cluster-reset \\ --cluster-reset-restore-path=<PATH-TO-SNAPSHOT> Result: A message in the logs says that RKE2 can be restarted without the flags. Start RKE2 again and should run successfully and be restored from the specified snapshot. When rke2 resets the cluster, it creates a file at /var/lib/rancher/rke2/server/db/etc/reset-file . If you want to reset the cluster again, you will need to delete this file. Options \u00b6 These options can be passed in with the command line, or in the configuration file. Which may be easier to use. Options Description --etcd-disable-snapshots Disable automatic etcd snapshots --etcd-snapshot-schedule-cron value Snapshot interval time in cron spec. eg. every 5 hours * */5 * * * (default: 0 */12 * * * ) --etcd-snapshot-retention value Number of snapshots to retain (default: 5) --etcd-snapshot-dir value Directory to save db snapshots. (Default location: ${data-dir}/db/snapshots ) --cluster-reset Forget all peers and become sole member of a new cluster. This can also be set with the environment variable [$K3S_CLUSTER_RESET] . --cluster-reset-restore-path value Path to snapshot file to be restored","title":"Etcd Backup and Restore"},{"location":"backup_restore/#etcd-backup-and-restore","text":"In this section, you'll learn how to create backups of the rke2 cluster data and to restore the cluster from backup. Note: /var/lib/rancher/rke2 is the default data directory for rke2, its however configurable via --data-dir","title":"Etcd Backup and Restore"},{"location":"backup_restore/#creating-snapshots","text":"Snapshots are enabled by default. The snapshot directory defaults to /var/lib/rancher/rke2/server/db/snapshots . To configure the snapshot interval or the number of retained snapshots, refer to the options.","title":"Creating Snapshots"},{"location":"backup_restore/#cluster-reset","text":"RKE2 enables a feature to reset the cluster to one member cluster by passing --cluster-reset flag, when passing this flag to rke2 server it will reset the cluster with the same data dir in place, the data directory for etcd exists in /var/lib/rancher/rke2/server/db/etcd , this flag can be passed in the events of quorum loss in the cluster. To pass the reset flag, first you need to stop RKE2 service if its enabled via systemd: systemctl stop rke2-server rke2 server --cluster-reset Result: A message in the logs say that RKE2 can be restarted without the flags. Start rke2 again and it should start rke2 as a 1 member cluster.","title":"Cluster Reset"},{"location":"backup_restore/#restoring-a-cluster-from-a-snapshot","text":"When RKE2 is restored from backup, the old data directory will be moved to /var/lib/rancher/rke2/server/db/etcd-old-%date%/ . Then RKE2 will attempt to restore the snapshot by creating a new data directory, then starting etcd with a new RKE2 cluster with one etcd member. To restore the cluster from backup, run RKE2 with the --cluster-reset option, with the --cluster-reset-restore-path also given: rke2 server \\ --cluster-reset \\ --cluster-reset-restore-path=<PATH-TO-SNAPSHOT> Result: A message in the logs says that RKE2 can be restarted without the flags. Start RKE2 again and should run successfully and be restored from the specified snapshot. When rke2 resets the cluster, it creates a file at /var/lib/rancher/rke2/server/db/etc/reset-file . If you want to reset the cluster again, you will need to delete this file.","title":"Restoring a Cluster from a Snapshot"},{"location":"backup_restore/#options","text":"These options can be passed in with the command line, or in the configuration file. Which may be easier to use. Options Description --etcd-disable-snapshots Disable automatic etcd snapshots --etcd-snapshot-schedule-cron value Snapshot interval time in cron spec. eg. every 5 hours * */5 * * * (default: 0 */12 * * * ) --etcd-snapshot-retention value Number of snapshots to retain (default: 5) --etcd-snapshot-dir value Directory to save db snapshots. (Default location: ${data-dir}/db/snapshots ) --cluster-reset Forget all peers and become sole member of a new cluster. This can also be set with the environment variable [$K3S_CLUSTER_RESET] . --cluster-reset-restore-path value Path to snapshot file to be restored","title":"Options"},{"location":"cgroups_v2/","text":"Control Groups V2 \u00b6 Linux distributions, more and more, are shipping with kernels and userspaces that support cgroups v2, e.g. since Fedora 31. However, at the time of this writing, the containerd that is built and shipped with RKE2 is a 1.3.x fork (with back-ported SELinux commits from 1.4.x) which does not support cgroups v2. Until RKE2 ships with containerd v1.4.x running it on cgroups v2 capable systems requires a little up-front configuration: Assuming a systemd -based system, setting the systemd.unified_cgroup_hierarchy=0 kernel parameter will indicate to systemd that it should run with hybrid (cgroups v1 + v2) support. Combined with the above, setting the systemd.legacy_systemd_cgroup_controller kernel parameter will indicate to systemd that it should run with legacy (cgroups v1) support. As these are kernel command-line arguments they must be set in the system bootloader so that they will be passed to systemd as PID 1 at /sbin/init . See: grub2 manual systemd manual cgroups v2","title":"Control Groups V2"},{"location":"cgroups_v2/#control-groups-v2","text":"Linux distributions, more and more, are shipping with kernels and userspaces that support cgroups v2, e.g. since Fedora 31. However, at the time of this writing, the containerd that is built and shipped with RKE2 is a 1.3.x fork (with back-ported SELinux commits from 1.4.x) which does not support cgroups v2. Until RKE2 ships with containerd v1.4.x running it on cgroups v2 capable systems requires a little up-front configuration: Assuming a systemd -based system, setting the systemd.unified_cgroup_hierarchy=0 kernel parameter will indicate to systemd that it should run with hybrid (cgroups v1 + v2) support. Combined with the above, setting the systemd.legacy_systemd_cgroup_controller kernel parameter will indicate to systemd that it should run with legacy (cgroups v1) support. As these are kernel command-line arguments they must be set in the system bootloader so that they will be passed to systemd as PID 1 at /sbin/init . See: grub2 manual systemd manual cgroups v2","title":"Control Groups V2"},{"location":"helm/","text":"Helm Integration \u00b6 Helm is the package management tool of choice for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents. With Helm we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/docs/intro/quickstart/ . RKE2 does not require any special configuration to use with Helm command-line tools. Just be sure you have properly set up your kubeconfig as per the section about cluster access . RKE2 does include some extra functionality to make deploying both traditional Kubernetes resource manifests and Helm Charts even easier with the rancher/helm-release CRD. This section covers the following topics: Automatically Deploying Manifests and Helm Charts Using the Helm CRD Customizing Packaged Components with HelmChartConfig Automatically Deploying Manifests and Helm Charts \u00b6 Any Kubernetes manifests found in /var/lib/rancher/rke2/server/manifests will automatically be deployed to RKE2 in a manner similar to kubectl apply . Manifests deployed in this manner are managed as AddOn custom resources, and can be viewed by running kubectl get addon -A . You will find AddOns for packaged components such as CoreDNS, Local-Storage, Traefik, etc. AddOns are created automatically by the deploy controller, and are named based on their filename in the manifests directory. It is also possible to deploy Helm charts as AddOns. RKE2 includes a Helm Controller that manages Helm charts using a HelmChart Custom Resource Definition (CRD). Using the Helm CRD \u00b6 The HelmChart resource definition captures most of the options you would normally pass to the helm command-line tool. Here's an example of how you might deploy Grafana from the default chart repository, overriding some of the default chart values. Note that the HelmChart resource itself is in the kube-system namespace, but the chart's resources will be deployed to the monitoring namespace. apiVersion : helm.cattle.io/v1 kind : HelmChart metadata : name : grafana namespace : kube-system spec : chart : stable/grafana targetNamespace : monitoring set : adminPassword : \"NotVerySafePassword\" valuesContent : |- image: tag: master env: GF_EXPLORE_ENABLED: true adminUser: admin sidecar: datasources: enabled: true HelmChart Field Definitions \u00b6 Field Default Description Helm Argument / Flag Equivalent name Helm Chart name NAME spec.chart Helm Chart name in repository, or complete HTTPS URL to chart archive (.tgz) CHART spec.targetNamespace default Helm Chart target namespace --namespace spec.version Helm Chart version (when installing from repository) --version spec.repo Helm Chart repository URL --repo spec.helmVersion v3 Helm version to use ( v2 or v3 ) spec.bootstrap False Set to True if this chart is needed to bootstrap the cluster (Cloud Controller Manager, etc) spec.set Override simple default Chart values. These take precedence over options set via valuesContent. --set / --set-string spec.valuesContent Override complex default Chart values via YAML file content --values spec.chartContent Base64-encoded chart archive .tgz - overrides spec.chart CHART Customizing Packaged Components with HelmChartConfig \u00b6 To allow overriding values for packaged components that are deployed as HelmCharts (such as Canal, CoreDNS, Ingress-Nginx, etc), RKE2 supports customizing deployments via a HelmChartConfig resources. The HelmChartConfig resource must match the name and namespace of its corresponding HelmChart, and supports providing additional valuesContent , which is passed to the helm command as an additional value file. Note: HelmChart spec.set values override HelmChart and HelmChartConfig spec.valuesContent settings. For example, to customize the packaged CoreDNS configuration, you can create a file named /var/lib/rancher/rke2/server/manifests/rke2-coredns-config.yaml and populate it with the following content: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-coredns namespace : kube-system spec : valuesContent : |- image: coredns/coredns imageTag: v1.7.1","title":"Helm Integration"},{"location":"helm/#helm-integration","text":"Helm is the package management tool of choice for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents. With Helm we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/docs/intro/quickstart/ . RKE2 does not require any special configuration to use with Helm command-line tools. Just be sure you have properly set up your kubeconfig as per the section about cluster access . RKE2 does include some extra functionality to make deploying both traditional Kubernetes resource manifests and Helm Charts even easier with the rancher/helm-release CRD. This section covers the following topics: Automatically Deploying Manifests and Helm Charts Using the Helm CRD Customizing Packaged Components with HelmChartConfig","title":"Helm Integration"},{"location":"helm/#automatically-deploying-manifests-and-helm-charts","text":"Any Kubernetes manifests found in /var/lib/rancher/rke2/server/manifests will automatically be deployed to RKE2 in a manner similar to kubectl apply . Manifests deployed in this manner are managed as AddOn custom resources, and can be viewed by running kubectl get addon -A . You will find AddOns for packaged components such as CoreDNS, Local-Storage, Traefik, etc. AddOns are created automatically by the deploy controller, and are named based on their filename in the manifests directory. It is also possible to deploy Helm charts as AddOns. RKE2 includes a Helm Controller that manages Helm charts using a HelmChart Custom Resource Definition (CRD).","title":"Automatically Deploying Manifests and Helm Charts"},{"location":"helm/#using-the-helm-crd","text":"The HelmChart resource definition captures most of the options you would normally pass to the helm command-line tool. Here's an example of how you might deploy Grafana from the default chart repository, overriding some of the default chart values. Note that the HelmChart resource itself is in the kube-system namespace, but the chart's resources will be deployed to the monitoring namespace. apiVersion : helm.cattle.io/v1 kind : HelmChart metadata : name : grafana namespace : kube-system spec : chart : stable/grafana targetNamespace : monitoring set : adminPassword : \"NotVerySafePassword\" valuesContent : |- image: tag: master env: GF_EXPLORE_ENABLED: true adminUser: admin sidecar: datasources: enabled: true","title":"Using the Helm CRD"},{"location":"helm/#helmchart-field-definitions","text":"Field Default Description Helm Argument / Flag Equivalent name Helm Chart name NAME spec.chart Helm Chart name in repository, or complete HTTPS URL to chart archive (.tgz) CHART spec.targetNamespace default Helm Chart target namespace --namespace spec.version Helm Chart version (when installing from repository) --version spec.repo Helm Chart repository URL --repo spec.helmVersion v3 Helm version to use ( v2 or v3 ) spec.bootstrap False Set to True if this chart is needed to bootstrap the cluster (Cloud Controller Manager, etc) spec.set Override simple default Chart values. These take precedence over options set via valuesContent. --set / --set-string spec.valuesContent Override complex default Chart values via YAML file content --values spec.chartContent Base64-encoded chart archive .tgz - overrides spec.chart CHART","title":"HelmChart Field Definitions"},{"location":"helm/#customizing-packaged-components-with-helmchartconfig","text":"To allow overriding values for packaged components that are deployed as HelmCharts (such as Canal, CoreDNS, Ingress-Nginx, etc), RKE2 supports customizing deployments via a HelmChartConfig resources. The HelmChartConfig resource must match the name and namespace of its corresponding HelmChart, and supports providing additional valuesContent , which is passed to the helm command as an additional value file. Note: HelmChart spec.set values override HelmChart and HelmChartConfig spec.valuesContent settings. For example, to customize the packaged CoreDNS configuration, you can create a file named /var/lib/rancher/rke2/server/manifests/rke2-coredns-config.yaml and populate it with the following content: apiVersion : helm.cattle.io/v1 kind : HelmChartConfig metadata : name : rke2-coredns namespace : kube-system spec : valuesContent : |- image: coredns/coredns imageTag: v1.7.1","title":"Customizing Packaged Components with HelmChartConfig"},{"location":"networking/","text":"Networking \u00b6 This page explains how CoreDNS, the Nginx-Ingress controller, and Klipper service load balancer work within RKE2. Refer to the Installation Network Options [FIXME] page for details on Canal configuration options, or how to set up your own CNI. For information on which ports need to be opened for RKE2, refer to the Installation Requirements [FIXME] . CoreDNS Nginx-Ingress Controller Service Load Balancer How the Service LB Works Usage Excluding the Service LB from Nodes Disabling the Service LB CoreDNS \u00b6 CoreDNS is deployed by default when starting the server. To disable, run each server with the --disable rke2-coredns option. If you don't install CoreDNS, you will need to install a cluster DNS provider yourself. Nginx Ingress Controller \u00b6 nginx-ingress is an Ingress controller that uses ConfigMap to store the nginx configuration. Nginx-ingress is deployed by default when starting the server. The ingress controller will use ports 80, and 443 on the host (i.e. these will not be usable for HostPort or NodePort). Nginx-ingress can be configured by creating a HelmChartConfig manifest to customize the rke2-nignix-ingress HelmChart values. For more information, refer to the official Traefik for Helm Configuration Parameters. To disable it, start each server with the --disable rke2-ingress-nginx option. Nodes Without a Hostname \u00b6 Some cloud providers, such as Linode, will create machines with \"localhost\" as the hostname and others may not have a hostname set at all. This can cause problems with domain name resolution. You can run RKE2 with the --node-name flag or RKE2_NODE_NAME environment variable and this will pass the node name to resolve this issue.","title":"Networking"},{"location":"networking/#networking","text":"This page explains how CoreDNS, the Nginx-Ingress controller, and Klipper service load balancer work within RKE2. Refer to the Installation Network Options [FIXME] page for details on Canal configuration options, or how to set up your own CNI. For information on which ports need to be opened for RKE2, refer to the Installation Requirements [FIXME] . CoreDNS Nginx-Ingress Controller Service Load Balancer How the Service LB Works Usage Excluding the Service LB from Nodes Disabling the Service LB","title":"Networking"},{"location":"networking/#coredns","text":"CoreDNS is deployed by default when starting the server. To disable, run each server with the --disable rke2-coredns option. If you don't install CoreDNS, you will need to install a cluster DNS provider yourself.","title":"CoreDNS"},{"location":"networking/#nginx-ingress-controller","text":"nginx-ingress is an Ingress controller that uses ConfigMap to store the nginx configuration. Nginx-ingress is deployed by default when starting the server. The ingress controller will use ports 80, and 443 on the host (i.e. these will not be usable for HostPort or NodePort). Nginx-ingress can be configured by creating a HelmChartConfig manifest to customize the rke2-nignix-ingress HelmChart values. For more information, refer to the official Traefik for Helm Configuration Parameters. To disable it, start each server with the --disable rke2-ingress-nginx option.","title":"Nginx Ingress Controller"},{"location":"networking/#nodes-without-a-hostname","text":"Some cloud providers, such as Linode, will create machines with \"localhost\" as the hostname and others may not have a hostname set at all. This can cause problems with domain name resolution. You can run RKE2 with the --node-name flag or RKE2_NODE_NAME environment variable and this will pass the node name to resolve this issue.","title":"Nodes Without a Hostname"},{"location":"architecture/architecture/","text":"Architecture Overview \u00b6 With RKE2 we take lessons learned from developing and maintaining our lightweight Kubernetes distribution, K3s , and apply them to build an enterprise-ready distribution with K3s ease-of-use. What this means is that RKE2 is, at its simplest, a single binary to be installed and configured on all nodes expected to participate in the Kubernetes cluster. Once started, RKE2 is then able to bootstrap and supervise role-appropriate agents per node while sourcing needed content from the network. RKE2 brings together a number of Open Source technologies to make this all work: K3s Helm Controller K8s API Server Controller Manager Kubelet Scheduler Proxy etcd runc containerd / cri CNI : Canal a.k.a. Calico & Flannel CoreDNS NGINX Ingress Controller Metrics Server Helm All of these, except the NGINX Ingress Controller, are compiled and statically linked with Go+BoringCrypto . Process Lifecycle \u00b6 Content Bootstrap \u00b6 RKE2 sources binaries and manifests to run both server and agent nodes from the RKE2 Runtime image. This means RKE2 scans /var/lib/rancher/rke2/agent/images/*.tar for the rancher/rke2-runtime image (with a tag correlating to the output of rke2 --version ) by default and if it cannot be found, attempts to pull it from the network (a.k.a. Docker Hub). RKE2 then extracts /bin/ from the image, flattening it into /var/lib/rancher/rke2/data/${RKE2_DATA_KEY}/bin where ${RKE2_DATA_KEY} represents a unique string identifying the image. For RKE2 to work as expected the runtime image must minimally provide: containerd (the CRI ) containerd-shim (shims wrap runc tasks and do not stop when containerd does) containerd-shim-runc-v1 containerd-shim-runc-v2 kubelet (the Kubernetes node agent) runc (the OCI runtime) The following ops tooling is also provided by the runtime image: ctr (low level containerd maintenance and inspection) crictl (low level CRI maintenance and inspection) kubectl (kubernetes cluster maintenance and inspection) socat (needed by containerd for port-forwarding) After the binaries have been extracted RKE2 will then extract /charts/ from the image into the /var/lib/rancher/rke2/server/manifests directory. Initialize Server \u00b6 In the embedded K3s engine servers are specialized agent processes which means that the following startup will be deferred until the node container runtime has started. Prepare Components \u00b6 kube-apiserver \u00b6 Pull the kube-apiserver image, if not present already, and spin up a goroutine to wait for etcd and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ . kube-controller-manager \u00b6 Pull the kube-controller-manager image, if not present already, and spin up a goroutine to wait for kube-apiserver and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ . kube-scheduler \u00b6 Pull the kube-scheduler image, if not present already, and spin up a goroutine to wait for kube-apiserver and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ . Start Cluster \u00b6 Spin up an HTTP server in a goroutine to listen for other cluster servers/agents then initialize/join the cluster. etcd \u00b6 Pull the etcd image, if not present already, and spin up a goroutine to wait for the kubelet and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ . helm-controller \u00b6 Spin up the goroutine to start the embedded helm-controller after waiting for kube-apiserver to be ready. Initialize Agent \u00b6 The agent process entry point. For server processes the embedded K3s engine invokes this directly. Container Runtime \u00b6 containerd \u00b6 Spawn the containerd process and listen for termination. If containerd exits then the rke2 process will also exit. Node Agent \u00b6 kubelet \u00b6 Spawn and supervise the kubelet process. If kubelet exits then rke2 will attempt to restart it. Once the kubelet is running it will start any available static pods. For servers this means that etcd and kube-apiserver will start, in succession, allowing the remaining components started via static pod to connect to the kube-apiserver and begin their processing. Server Charts \u00b6 On server nodes, the helm-controller can now apply to the cluster any charts found in /var/lib/rancher/rke2/server/manifests . rke2-canal.yaml (daemonset, bootstrap) rke2-coredns.yaml (deployment, bootstrap) rke2-ingress-nginx.yaml (deployment) rke2-kube-proxy.yaml (daemonset, bootstrap) rke2-metrics-server.yaml (deployment) Daemon Process \u00b6 The RKE2 process will now run indefinitely until it receives a SIGTERM or SIGKILL or if the containerd process exits.","title":"Anatomy of a Next Generation Kubernetes Distribution"},{"location":"architecture/architecture/#architecture-overview","text":"With RKE2 we take lessons learned from developing and maintaining our lightweight Kubernetes distribution, K3s , and apply them to build an enterprise-ready distribution with K3s ease-of-use. What this means is that RKE2 is, at its simplest, a single binary to be installed and configured on all nodes expected to participate in the Kubernetes cluster. Once started, RKE2 is then able to bootstrap and supervise role-appropriate agents per node while sourcing needed content from the network. RKE2 brings together a number of Open Source technologies to make this all work: K3s Helm Controller K8s API Server Controller Manager Kubelet Scheduler Proxy etcd runc containerd / cri CNI : Canal a.k.a. Calico & Flannel CoreDNS NGINX Ingress Controller Metrics Server Helm All of these, except the NGINX Ingress Controller, are compiled and statically linked with Go+BoringCrypto .","title":"Architecture Overview"},{"location":"architecture/architecture/#process-lifecycle","text":"","title":"Process Lifecycle"},{"location":"architecture/architecture/#content-bootstrap","text":"RKE2 sources binaries and manifests to run both server and agent nodes from the RKE2 Runtime image. This means RKE2 scans /var/lib/rancher/rke2/agent/images/*.tar for the rancher/rke2-runtime image (with a tag correlating to the output of rke2 --version ) by default and if it cannot be found, attempts to pull it from the network (a.k.a. Docker Hub). RKE2 then extracts /bin/ from the image, flattening it into /var/lib/rancher/rke2/data/${RKE2_DATA_KEY}/bin where ${RKE2_DATA_KEY} represents a unique string identifying the image. For RKE2 to work as expected the runtime image must minimally provide: containerd (the CRI ) containerd-shim (shims wrap runc tasks and do not stop when containerd does) containerd-shim-runc-v1 containerd-shim-runc-v2 kubelet (the Kubernetes node agent) runc (the OCI runtime) The following ops tooling is also provided by the runtime image: ctr (low level containerd maintenance and inspection) crictl (low level CRI maintenance and inspection) kubectl (kubernetes cluster maintenance and inspection) socat (needed by containerd for port-forwarding) After the binaries have been extracted RKE2 will then extract /charts/ from the image into the /var/lib/rancher/rke2/server/manifests directory.","title":"Content Bootstrap"},{"location":"architecture/architecture/#initialize-server","text":"In the embedded K3s engine servers are specialized agent processes which means that the following startup will be deferred until the node container runtime has started.","title":"Initialize Server"},{"location":"architecture/architecture/#prepare-components","text":"","title":"Prepare Components"},{"location":"architecture/architecture/#kube-apiserver","text":"Pull the kube-apiserver image, if not present already, and spin up a goroutine to wait for etcd and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ .","title":"kube-apiserver"},{"location":"architecture/architecture/#kube-controller-manager","text":"Pull the kube-controller-manager image, if not present already, and spin up a goroutine to wait for kube-apiserver and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ .","title":"kube-controller-manager"},{"location":"architecture/architecture/#kube-scheduler","text":"Pull the kube-scheduler image, if not present already, and spin up a goroutine to wait for kube-apiserver and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ .","title":"kube-scheduler"},{"location":"architecture/architecture/#start-cluster","text":"Spin up an HTTP server in a goroutine to listen for other cluster servers/agents then initialize/join the cluster.","title":"Start Cluster"},{"location":"architecture/architecture/#etcd","text":"Pull the etcd image, if not present already, and spin up a goroutine to wait for the kubelet and then write the static pod definition in /var/lib/rancher/rke2/agent/pod-manifests/ .","title":"etcd"},{"location":"architecture/architecture/#helm-controller","text":"Spin up the goroutine to start the embedded helm-controller after waiting for kube-apiserver to be ready.","title":"helm-controller"},{"location":"architecture/architecture/#initialize-agent","text":"The agent process entry point. For server processes the embedded K3s engine invokes this directly.","title":"Initialize Agent"},{"location":"architecture/architecture/#container-runtime","text":"","title":"Container Runtime"},{"location":"architecture/architecture/#containerd","text":"Spawn the containerd process and listen for termination. If containerd exits then the rke2 process will also exit.","title":"containerd"},{"location":"architecture/architecture/#node-agent","text":"","title":"Node Agent"},{"location":"architecture/architecture/#kubelet","text":"Spawn and supervise the kubelet process. If kubelet exits then rke2 will attempt to restart it. Once the kubelet is running it will start any available static pods. For servers this means that etcd and kube-apiserver will start, in succession, allowing the remaining components started via static pod to connect to the kube-apiserver and begin their processing.","title":"kubelet"},{"location":"architecture/architecture/#server-charts","text":"On server nodes, the helm-controller can now apply to the cluster any charts found in /var/lib/rancher/rke2/server/manifests . rke2-canal.yaml (daemonset, bootstrap) rke2-coredns.yaml (deployment, bootstrap) rke2-ingress-nginx.yaml (deployment) rke2-kube-proxy.yaml (daemonset, bootstrap) rke2-metrics-server.yaml (deployment)","title":"Server Charts"},{"location":"architecture/architecture/#daemon-process","text":"The RKE2 process will now run indefinitely until it receives a SIGTERM or SIGKILL or if the containerd process exits.","title":"Daemon Process"},{"location":"install/airgap/","text":"Air-Gap Install \u00b6 RKE2 can be installed in an air-gapped environment with two different methods. You can either deploy via the bundled rke2-airgap-images tarball, or by using a private registry. All files mentioned in the steps can be obtained from the assets of the desired released rke2 version here . If running on an SELinux enforcing air-gapped node, you must first install the necessary SELinux policy RPM before performing these steps. See our RPM Documentation to determine what you need. Tarball Method \u00b6 Add the desired version of the rke2-airgap-images-amd64.tar.gz file to the air-gapped node. Gunzip the tar.gz file so that it is only a tar, and move it to /var/lib/rancher/rke2/agent/images/ . Install RKE2 Private Registry Method \u00b6 The private registry must be using TLS, with a cert trusted by the host CA bundle. If the registry is using a self-signed cert, you can add the cert to the host CA bundle with update-ca-certificates . The registry must also allow anonymous (unauthenticated) access. Add all the required system images to your private registry. A simple list of these can be obtained from the rke2-images.linux-amd64.txt file. Add the ca cert to the operating system's trusted certs Install RKE2 using the --system-default-registry flag. Install RKE2 \u00b6 These steps should only be performed after completing one of either the Tarball Method or Private Registry Method . Obtain the rke2 binary file rke2.linux-amd64 Ensure the binary is named rke2 and place it in /usr/local/bin . Ensure it is executable. Run the binary with the desired flags. For example, if using the Private Registry Method: sudo rke2 server --system-default-registry=\"https://myprivreg.com:5000\" 2>&1 &","title":"Air-Gap Install"},{"location":"install/airgap/#air-gap-install","text":"RKE2 can be installed in an air-gapped environment with two different methods. You can either deploy via the bundled rke2-airgap-images tarball, or by using a private registry. All files mentioned in the steps can be obtained from the assets of the desired released rke2 version here . If running on an SELinux enforcing air-gapped node, you must first install the necessary SELinux policy RPM before performing these steps. See our RPM Documentation to determine what you need.","title":"Air-Gap Install"},{"location":"install/airgap/#tarball-method","text":"Add the desired version of the rke2-airgap-images-amd64.tar.gz file to the air-gapped node. Gunzip the tar.gz file so that it is only a tar, and move it to /var/lib/rancher/rke2/agent/images/ . Install RKE2","title":"Tarball Method"},{"location":"install/airgap/#private-registry-method","text":"The private registry must be using TLS, with a cert trusted by the host CA bundle. If the registry is using a self-signed cert, you can add the cert to the host CA bundle with update-ca-certificates . The registry must also allow anonymous (unauthenticated) access. Add all the required system images to your private registry. A simple list of these can be obtained from the rke2-images.linux-amd64.txt file. Add the ca cert to the operating system's trusted certs Install RKE2 using the --system-default-registry flag.","title":"Private Registry Method"},{"location":"install/airgap/#install-rke2","text":"These steps should only be performed after completing one of either the Tarball Method or Private Registry Method . Obtain the rke2 binary file rke2.linux-amd64 Ensure the binary is named rke2 and place it in /usr/local/bin . Ensure it is executable. Run the binary with the desired flags. For example, if using the Private Registry Method: sudo rke2 server --system-default-registry=\"https://myprivreg.com:5000\" 2>&1 &","title":"Install RKE2"},{"location":"install/quickstart/","text":"This guide will help you quickly launch a cluster with default options. For information on how RKE2 components work together, refer to the architecture section. New to Kubernetes? The official Kubernetes docs already have some great tutorials outlining the basics here . Server Node Installation \u00b6 RKE2 provides an installation script that is a convenient way to install it as a service on systemd based systems. This script is available at https://get.rke2.io. To install RKE2 using this method do the following: Run the installer curl -sfL https://get.rke2.io | sh - This will install the rke2-server service and the rke2 binary onto your machine. Enable the rke2-server service systemctl enable rke2-server.service Start the service systemctl start rke2-server.service Follow the logs, if you like journalctl -u rke2-server -f After running this installation: The rke2-server service will be installed. The rke2-server service will be configured to automatically restart after node reboots or if the process crashes or is killed. Additional utilities will be installed at /var/lib/rancher/rke2/bin/ . They include: kubectl , crictl , and ctr . Note that these are not on your path by default. Two cleanup scripts will be installed to the path at /usr/local/bin/rke2 . They are: rke2-killall.sh and rke2-uninstall.sh . A kubeconfig file will be written to /etc/rancher/rke2/rke2.yaml . A token that can be used to register other server or agent nodes will be created at /var/lib/rancher/rke2/server/node-token Note: If you are adding additional server nodes, you must have an odd number in total. An odd number is needed to maintain quorom. Worker Node Configuration \u00b6 Run the installer curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=\"agent\" sh - This will install the rke2-agent service and the rke2 binary onto your machine. Enable the rke2-agent service systemctl enable rke2-agent.service Configure the rke2-agent service mkdir -p /etc/rancher/rke2/ vim /etc/rancher/rke2/config.yaml Content for config.yaml: server: https://<server>:9345 token: <token from server node> Start the service systemctl start rke2-agent.service Follow the logs, if you like journalctl -u rke2-agent -f Note: Each machine must have a unique hostname. If your machines do not have unique hostnames, set the node-name parameter in the config.yaml file and provide a value with a valid and unique hostname for each node.","title":"Quick Start"},{"location":"install/quickstart/#server-node-installation","text":"RKE2 provides an installation script that is a convenient way to install it as a service on systemd based systems. This script is available at https://get.rke2.io. To install RKE2 using this method do the following: Run the installer curl -sfL https://get.rke2.io | sh - This will install the rke2-server service and the rke2 binary onto your machine. Enable the rke2-server service systemctl enable rke2-server.service Start the service systemctl start rke2-server.service Follow the logs, if you like journalctl -u rke2-server -f After running this installation: The rke2-server service will be installed. The rke2-server service will be configured to automatically restart after node reboots or if the process crashes or is killed. Additional utilities will be installed at /var/lib/rancher/rke2/bin/ . They include: kubectl , crictl , and ctr . Note that these are not on your path by default. Two cleanup scripts will be installed to the path at /usr/local/bin/rke2 . They are: rke2-killall.sh and rke2-uninstall.sh . A kubeconfig file will be written to /etc/rancher/rke2/rke2.yaml . A token that can be used to register other server or agent nodes will be created at /var/lib/rancher/rke2/server/node-token Note: If you are adding additional server nodes, you must have an odd number in total. An odd number is needed to maintain quorom.","title":"Server Node Installation"},{"location":"install/quickstart/#worker-node-configuration","text":"Run the installer curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=\"agent\" sh - This will install the rke2-agent service and the rke2 binary onto your machine. Enable the rke2-agent service systemctl enable rke2-agent.service Configure the rke2-agent service mkdir -p /etc/rancher/rke2/ vim /etc/rancher/rke2/config.yaml Content for config.yaml: server: https://<server>:9345 token: <token from server node> Start the service systemctl start rke2-agent.service Follow the logs, if you like journalctl -u rke2-agent -f Note: Each machine must have a unique hostname. If your machines do not have unique hostnames, set the node-name parameter in the config.yaml file and provide a value with a valid and unique hostname for each node.","title":"Worker Node Configuration"},{"location":"security/cis_self_assessment/","text":"CIS Kubernetes Benchmark v1.5 - RKE2 v1.18 \u00b6 Overview \u00b6 This document is a companion to the RKE2 security hardening guide. The hardening guide provides prescriptive guidance for hardening a production installation of RKE2, and this benchmark guide is meant to help you evaluate the level of security of the hardened cluster against each control in the CIS Kubernetes benchmark. It is to be used by RKE2 operators, security teams, auditors, and decision makers. This guide is specific to the v1.18 release line of RKE2 and the v1.5.1 release of the CIS Kubernetes Benchmark. For more detail about each control, including more detailed descriptions and remediations for failing tests, you can refer to the corresponding section of the CIS Kubernetes Benchmark v1.5. You can download the benchmark after logging in to CISecurity.org . Testing controls methodology \u00b6 Each control in the CIS Kubernetes Benchmark was evaluated against an RKE2 cluster that was configured according to the accompanying hardening guide. Where control audits differ from the original CIS benchmark, the audit commands specific to RKE2 are provided for testing. These are the possible results for each control: Pass - The RKE2 cluster under test passed the audit outlined in the benchmark. Not Applicable - The control is not applicable to RKE2 because of how it is designed to operate. The remediation section will explain why this is so. Not Scored - Operator Dependent - The control is not scored in the CIS benchmark and it depends on the cluster's use case or some other factor that must be determined by the cluster operator. These controls have been evaluated to ensure RKE2 does not prevent their implementation, but no further configuration or auditing of the cluster under test has been performed. Controls \u00b6 1 Master Node Security Configuration \u00b6 1.1 Master Node Configuration Files \u00b6 1.1.1 \u00b6 Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Scored) Rationale The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-apiserver.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed. 1.1.2 \u00b6 Ensure that the API server pod specification file ownership is set to root:root (Scored) Rationale The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root . Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-apiserver.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed. 1.1.3 \u00b6 Ensure that the controller manager pod specification file permissions are set to 644 or more restrictive (Scored) Rationale The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-controller-manager.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed. 1.1.4 \u00b6 Ensure that the controller manager pod specification file ownership is set to root:root (Scored) Rationale The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-controller-manager.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed. 1.1.5 \u00b6 Ensure that the scheduler pod specification file permissions are set to 644 or more restrictive (Scored) Rationale The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed. 1.1.6 \u00b6 Ensure that the scheduler pod specification file ownership is set to root:root (Scored) Rationale The scheduler pod specification file controls various parameters that set the behavior of the kube-scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed. 1.1.7 \u00b6 Ensure that the etcd pod specification file permissions are set to 644 or more restrictive (Scored) Rationale The etcd pod specification file /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly- available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed. 1.1.8 \u00b6 Ensure that the etcd pod specification file ownership is set to root:root (Scored) Rationale The etcd pod specification file /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly- available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed. 1.1.9 \u00b6 Ensure that the Container Network Interface file permissions are set to 644 or more restrictive (Not Scored) Rationale Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/manifests/rke2-canal.yml 644 Remediation: RKE2 deploys the default CNI, Canal, using a Helm chart. The chart is defined as a custom resource in a file with 644 permissions. No manual remediation needed. 1.1.10 \u00b6 Ensure that the Container Network Interface file ownership is set to root:root (Not Scored) Rationale Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/manifests/rke2-canal.yml root:root Remediation: RKE2 deploys the default CNI, Canal, using a Helm chart. The chart is defined as a custom resource in a file with root:root ownership. No manual remediation needed. 1.1.11 \u00b6 Ensure that the etcd data directory permissions are set to 700 or more restrictive (Scored) Rationale etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/db/etcd 700 Remediation: RKE2 manages the etcd data directory and sets its permissions to 700. No manual remediation needed. 1.1.12 \u00b6 Ensure that the etcd data directory ownership is set to etcd:etcd (Scored) Rationale etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/db/etcd etcd:etcd Remediation: When running RKE2 with the profile flag set to cis-1.5 , RKE2 will refuse to start if the etcd user and group doesn't exist on the host. If it does exist, RKE2 will automatically set the ownership of the etcd data directory to etcd:etcd and ensure the etcd static pod is started with that user and group. 1.1.13 \u00b6 Ensure that the admin.conf file permissions are set to 644 or more restrictive (Scored) Rationale The admin.conf is the administrator kubeconfig file defining various settings for the administration of the cluster. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/admin.kubeconfig . Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/admin.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/admin.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed. 1.1.14 \u00b6 Ensure that the admin.conf file ownership is set to root:root (Scored) Rationale The admin.conf file contains the admin credentials for the cluster. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/admin.kubeconfig . Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/admin.kubeconfig root:root Remediation: By default, RKE2 creates this file at stat -c %U:%G /var/lib/rancher/rke2/server/cred/admin.kubeconfig and automatically sets its ownership to root:root . 1.1.15 \u00b6 Ensure that the scheduler.conf file permissions are set to 644 or more restrictive (Scored) Rationale The scheduler.conf file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig . Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed. 1.1.16 \u00b6 Ensure that the scheduler.conf file ownership is set to root:root (Scored) Rationale The scheduler.conf file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig . Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig root:root Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig and automatically sets its ownership to root:root . 1.1.17 \u00b6 Ensure that the controller.kubeconfig file permissions are set to 644 or more restrictive (Scored) Rationale The controller.kubeconfig file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/controller.kubeconfig . Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/controller.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/controller.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed. 1.1.18 \u00b6 Ensure that the controller.kubeconfig file ownership is set to root:root (Scored) Rationale The controller.kubeconfig file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/controller.kubeconfig . Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/controller.kubeconfig root:root Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/controller.kubeconfig and automatically sets its ownership to root:root . 1.1.19 \u00b6 Ensure that the Kubernetes PKI directory and file ownership is set to root:root (Scored) Rationale Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/tls root:root Remediation: By default, RKE2 creates the directory and files with the expected ownership of root:root . No manual remediation should be necessary. 1.1.20 \u00b6 Ensure that the Kubernetes PKI certificate file permissions are set to 644 or more restrictive (Scored) Rationale Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 644 or more restrictive to protect their integrity. Result: Pass Audit: Run the below command on the master node. stat -c %n \\ %a /var/lib/rancher/rke2/server/tls/*.crt Verify that the permissions are 644 or more restrictive. Remediation: By default, RKE2 creates the files with the expected permissions of 644 . No manual remediation is needed. 1.1.21 \u00b6 Ensure that the Kubernetes PKI key file permissions are set to 600 (Scored) Rationale Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality. Result: Pass Audit: Run the below command on the master node. stat -c %n \\ %a /var/lib/rancher/rke2/server/tls/*.key Verify that the permissions are 600 or more restrictive. Remediation: By default, RKE2 creates the files with the expected permissions of 600 . No manual remediation is needed. 1.2 API Server \u00b6 This section contains recommendations relating to API server configuration flags 1.2.1 \u00b6 Ensure that the --anonymous-auth argument is set to false (Not Scored) Rationale When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that --anonymous-auth=false is present. Remediation: By default, RKE2 kube-apiserver is configured to run with this flag and value. No manual remediation is needed. 1.2.2 \u00b6 Ensure that the --basic-auth-file argument is not set (Scored) Rationale Basic authentication uses plaintext credentials for authentication. Currently, the basic authentication credentials last indefinitely, and the password cannot be changed without restarting the API server. The basic authentication is currently supported for convenience. Hence, basic authentication should not be used. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --basic-auth-file argument does not exist. Remediation: By default, RKE2 does not run with basic authentication enabled. No manual remediation is needed. 1.2.3 \u00b6 Ensure that the --token-auth-file parameter is not set (Scored) Rationale The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token-based authentication. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --basic-auth-file argument does not exist. Remediation: By default, RKE2 does not run with basic authentication enabled. No manual remediation is needed. 1.2.4 \u00b6 Ensure that the --kubelet-https argument is set to true (Scored) Rationale Connections from apiserver to kubelets could potentially carry sensitive data such as secrets and keys. It is thus important to use in-transit encryption for any communication between the apiserver and kubelets. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-https argument does not exist. Remediation: By default, RKE2 kube-apiserver doesn't run with the --kubelet-https parameter as it runs with TLS. No manual remediation is needed. 1.2.5 \u00b6 Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate (Scored) Rationale The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate. Remediation: By default, RKE2 kube-apiserver is ran with these arguments for secure communication with kubelet. No manual remediation is needed. 1.2.6 \u00b6 Ensure that the --kubelet-certificate-authority argument is set as appropriate (Scored) Rationale The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-certificate-authority argument exists and is set as appropriate. Remediation: By default, RKE2 kube-apiserver is ran with this argument for secure communication with kubelet. No manual remediation is needed. 1.2.7 \u00b6 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored) Rationale The API Server, can be configured to allow all requests. This mode should not be used on any production cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the argument value doesn't contain AlwaysAllow . Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed. 1.2.8 \u00b6 Ensure that the --authorization-mode argument includes Node (Scored) Rationale The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify Node exists as a parameter to the argument. Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed. 1.2.9 \u00b6 Ensure that the --authorization-mode argument includes RBAC (Scored) Rationale Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify RBAC exists as a parameter to the argument. Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed. 1.2.10 \u00b6 Ensure that the admission control plugin EventRateLimit is set (Not Scored) Rationale Using EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept. Note: This is an Alpha feature in the Kubernetes 1.15 release. Result: Not Scored - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes EventRateLimit. Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. To configure this, follow the Kubernetes documentation and set the desired limits in a configuration file. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter. 1.2.11 \u00b6 Ensure that the admission control plugin AlwaysAdmit is not set (Scored) Rationale Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that if the --enable-admission-plugins argument is set, its value does not include AlwaysAdmit . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed. 1.2.12 \u00b6 Ensure that the admission control plugin AlwaysPullImages is set (Not Scored) Rationale Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image\u2019s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. Result: Not Scored - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes AlwaysPullImages . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. To configure this, follow the Kubernetes documentation and set the desired limits in a configuration file. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter. 1.2.13 \u00b6 Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used (Not Scored) Rationale SecurityContextDeny can be used to provide a layer of security for clusters which do not have PodSecurityPolicies enabled. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes SecurityContextDeny , if PodSecurityPolicy is not included. Remediation: By default, RKE2 automatically enables the PodSecurityPolicy admission plugin. Therefore, the SecurityContextDeny plugin need not be enabled. No manual remediation needed. 1.2.14 \u00b6 Ensure that the admission control plugin ServiceAccount is set (Scored) Rationale When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --disable-admission-plugins argument is set to a value that does not includes ServiceAccount . Remediation: By default, RKE2 does not use this argument. If there's a desire to use this argument, follow the documentation and create ServiceAccount objects as per your environment. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter. 1.2.15 \u00b6 Ensure that the admission control plugin NamespaceLifecycle is set (Scored) Rationale Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --disable-admission-plugins argument is set to a value that does not include NamespaceLifecycle . Remediation: By default, RKE2 does not use this argument. No manual remediation needed. 1.2.16 \u00b6 Ensure that the admission control plugin PodSecurityPolicy is set (Scored) Rationale A Pod Security Policy is a cluster-level resource that controls the actions that a pod can perform and what it has the ability to access. The PodSecurityPolicy objects define a set of conditions that a pod must run with in order to be accepted into the system. Pod Security Policies are comprised of settings and strategies that control the security features a pod has access to and hence this must be used to control pod access permissions. Note: When the PodSecurityPolicy admission plugin is in use, there needs to be at least one PodSecurityPolicy in place for ANY pods to be admitted. See section 1.7 for recommendations on PodSecurityPolicy settings. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes PodSecurityPolicy . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed. 1.2.17 \u00b6 Ensure that the admission control plugin NodeRestriction is set (Scored) Rationale Using the NodeRestriction plug-in ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed. 1.2.18 \u00b6 Ensure that the --insecure-bind-address argument is not set (Scored) Rationale If you bind the apiserver to an insecure address, basically anyone who could connect to it over the insecure port, would have unauthenticated and unencrypted access to your master node. The apiserver doesn't do any authentication checking for insecure binds and traffic to the Insecure API port is not encrpyted, allowing attackers to potentially read sensitive data in transit. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --insecure-bind-address argument does not exist. Remediation: By default, RKE2 explicitly excludes the use of the --insecure-bind-address parameter. No manual remediation is needed. 1.2.19 \u00b6 Ensure that the --insecure-port argument is set to 0 (Scored) Rationale Setting up the apiserver to serve on an insecure port would allow unauthenticated and unencrypted access to your master node. This would allow attackers who could access this port, to easily take control of the cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --insecure-port argument is set to 0. Remediation: By default, RKE2 starts the kube-apiserver process with this argument's parameter set to 0. No manual remediation is needed. 1.2.20 \u00b6 Ensure that the --secure-port argument is not set to 0 (Scored) Rationale The secure port is used to serve https with authentication and authorization. If you disable it, no https traffic is served and all traffic is served unencrypted. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --secure-port argument is either not set or is set to an integer value between 1 and 65535. Remediation: By default, RKE2 sets the parameter of 6443 for the --secure-port argument. No manual remediation is needed. 1.2.21 \u00b6 Ensure that the --profiling argument is set to false (Scored) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed. 1.2.22 \u00b6 Ensure that the --audit-log-path argument is set (Scored) Rationale Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-path argument is set as appropriate. Remediation: By default, RKE2 sets the --audit-log-path argument and parameter. No manual remediation needed. 1.2.23 \u00b6 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Scored) Rationale Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxage argument is set to 30 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxage argument parameter to 30. No manual remediation needed. 1.2.24 \u00b6 Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Scored) Rationale Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxbackup argument parameter to 10. No manual remediation needed. 1.2.25 \u00b6 Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Scored) Rationale Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxsize argument is set to 100 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxsize argument parameter to 100. No manual remediation needed. 1.2.26 \u00b6 Ensure that the --request-timeout argument is set as appropriate (Scored) Rationale Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --request-timeout argument is either not set or set to an appropriate value. Remediation: By default, RKE2 does not set the --request-timeout argument. No manual remediation needed. 1.2.27 \u00b6 Ensure that the --service-account-lookup argument is set to true (Scored) Rationale If --service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that if the --service-account-lookup argument exists it is set to true. Remediation: By default, RKE2 doesn't set this argument in favor of taking the default effect. No manual remediation needed. 1.2.28 \u00b6 Ensure that the --service-account-key-file argument is set as appropriate (Scored) Rationale By default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file . Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --service-account-key-file argument exists and is set as appropriate. Remediation: By default, RKE2 sets the --service-account-key-file explicitly. No manual remediation needed. 1.2.29 \u00b6 Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate. Remediation: By default, RKE2 sets the --etcd-certfile and --etcd-keyfile arguments explicitly. No manual remediation needed. 1.2.30 \u00b6 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Scored) Rationale API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate. Remediation: By default, RKE2 sets the --tls-cert-file and --tls-private-key-file arguments explicitly. No manual remediation needed. 1.2.31 \u00b6 Ensure that the --client-ca-file argument is set as appropriate (Scored) Rationale API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If --client-ca-file argument is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --client-ca-file argument exists and it is set as appropriate. Remediation: By default, RKE2 sets the --client-ca-file argument explicitly. No manual remediation needed. 1.2.32 \u00b6 Ensure that the --etcd-cafile argument is set as appropriate (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --etcd-cafile argument exists and it is set as appropriate. Remediation: By default, RKE2 sets the --etcd-cafile argument explicitly. No manual remediation needed. 1.2.33 \u00b6 Ensure that the --encryption-provider-config argument is set as appropriate (Scored) Rationale etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --encryption-provider-config argument is set to a EncryptionConfigfile. Additionally, ensure that the EncryptionConfigfile has all the desired resources covered especially any secrets. Remediation: By default, RKE2 sets the --encryption-provider-config argument explicitly. No manual remediation needed. RKE2's default encryption provider config file is located at /var/lib/rancher/rke2/server/cred/encryption-config.json and is configured to encrypt secrets. 1.2.34 \u00b6 Ensure that encryption providers are appropriately configured (Scored) Rationale Where etcd encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the aescbc , kms and secretbox are likely to be appropriate options. Result: Pass Remediation: Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc , kms or secretbox as the encryption provider. Audit: Run the below command on the master node. grep aescbc /var/lib/rancher/rke2/server/cred/encryption-config.json Run the below command on the master node. Verify that aescbc is set as the encryption provider for all the desired resources. Remediation By default, RKE2 sets the argument --encryption-provider-config and parameter. The contents of the config file indicates the use of aescbc. No manual remediation needed. 1.2.35 \u00b6 Ensure that the API Server only makes use of Strong Cryptographic Ciphers (Not Scored) Rationale TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Result: Not Scored - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --tls-cipher-suites argument is set as outlined in the remediation procedure below. Remediation: By default, RKE2 explicitly doesn't set this flag. No manual remediation needed. 1.3 Controller Manager \u00b6 1.3.1 \u00b6 Ensure that the --terminated-pod-gc-threshold argument is set as appropriate (Not Scored) Rationale Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection. Result: Not Scored - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --terminated-pod-gc-threshold argument is set as appropriate. Remediation: By default, RKE2 sets the --terminated-pod-gc-threshold argument with a value of 1000. No manual remediation needed. 1.3.2 \u00b6 Ensure that the --profiling argument is set to false (Scored) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed. 1.3.3 \u00b6 Ensure that the --use-service-account-credentials argument is set to true (Scored) Rationale The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-service-account-credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --use-service-account-credentials argument is set to true. Remediation: By default, RKE2 sets the --use-service-account-credentials argument to true. No manual remediation needed. 1.3.4 \u00b6 Ensure that the --service-account-private-key-file argument is set as appropriate (Scored) Rationale To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private-key-file as appropriate. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --service-account-private-key-file argument is set as appropriate. Remediation: By default, RKE2 sets the --service-account-private-key-file argument with the service account key file. No manual remediation needed. 1.3.5 \u00b6 Ensure that the --root-ca-file argument is set as appropriate (Scored) Rationale Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate Remediation: By default, RKE2 sets the --root-ca-file argument with the root ca file. No manual remediation needed. 1.3.6 \u00b6 Ensure that the RotateKubeletServerCertificate argument is set to true (Scored) Rationale RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Result: Not Applicable Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that RotateKubeletServerCertificateargument exists and is set to true. Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation. 1.3.7 \u00b6 Ensure that the --bind-address argument is set to 127.0.0.1 (Scored) Rationale The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --bind-address argument is set to 127.0.0.1. Remediation: By default, RKE2 sets the --bind-address argument to 127.0.0.1 . No manual remediation needed. 1.4 Scheduler \u00b6 This section contains recommendations relating to Scheduler configuration flags 1.4.1 \u00b6 Ensure that the --profiling argument is set to false (Scored) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-scheduler | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed. 1.4.2 \u00b6 Ensure that the --bind-address argument is set to 127.0.0.1 (Scored) Rationale The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-scheduler | grep -v grep Verify that the --bind-address argument is set to 127.0.0.1. Remediation: By default, RKE2 sets the --bind-address argument to 127.0.0.1 . No manual remediation needed. 2 Etcd Node Configuration \u00b6 This section covers recommendations for etcd configuration. 2.1 \u00b6 Ensure that the cert-file and key-file fields are set as appropriate (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Result: Pass Audit: Run the below command on the master node. grep -E 'cert-file|key-file' /var/lib/rancher/rke2/server/db/etcd/config Verify that the cert-file and the key-file fields are set as appropriate. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Server and peer cert and key files are specified. No manual remediation needed. 2.2 \u00b6 Ensure that the client-cert-auth field is set to true (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Result: Pass Audit: Run the below command on the master node. grep 'client-cert-auth' /var/lib/rancher/rke2/server/db/etcd/config Verify that the client-cert-auth field is set to true. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . client-cert-auth is set to true. No manual remediation needed. 2.3 \u00b6 Ensure that the auto-tls field is not set to true (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Result: Pass Audit: Run the below command on the master node. grep 'auto-tls' /var/lib/rancher/rke2/server/db/etcd/config Verify that if the auto-tls field does not exist. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, it does not contain the auto-tls argument. No manual remediation needed. 2.4 \u00b6 Ensure that the peer-cert-file and peer-key-file fields are set as appropriate (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters. Result: Pass Audit: Run the below command on the master node. grep -E 'peer-server-client.crt|peer-server-client.key' /var/lib/rancher/rke2/server/db/etcd/config Verify that the peer-server-client.crt and peer-server-client.key fields are set as appropriate. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, the peer-server-client.crt and peer-server-client.key fields are set. No manual remediation needed. 2.5 \u00b6 Ensure that the client-cert-auth field is set to true (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Result: Not Applicable Audit: Run the below command on the master node. grep 'client-cert-auth' /var/lib/rancher/rke2/server/db/etcd/config Verify that the client-cert-auth field in the peer section is set to true. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, the client-cert-auth field is set. No manual remediation needed. 2.6 \u00b6 Ensure that the peer-auto-tls field is not set to true (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self- signed certificates for authentication. Result: Pass Audit: Run the below command on the master node. grep 'peer-auto-tls' /var/lib/rancher/rke2/server/db/etcd/config Verify that if the peer-auto-tls field does not exist. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, it does not contain the peer-auto-tls field. No manual remediation needed. 2.7 \u00b6 Ensure that a unique Certificate Authority is used for etcd (Not Scored) Rationale etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database. Result: Pass Audit: Run the below command on the master node. # To find the ca file used by etcd: grep 'trusted-ca-file' /var/lib/rancher/rke2/server/db/etcd/config # To find the kube-apiserver process: /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the file referenced by the client-ca-file flag in the apiserver process is different from the file referenced by the trusted-ca-file parameter in the etcd configuration file. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config and the trusted-ca-file parameters in it are set to unique values specific to etcd. No manual remediation needed. 3 Control Plane Configuration \u00b6 3.1 Authentication and Authorization \u00b6 3.1.1 \u00b6 Client certificate authentication should not be used for users (Not Scored) Rationale With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Result: Not Scored - Operator Dependent Audit: Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication. Remediation: Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates. 3.2 Logging \u00b6 3.2.1 \u00b6 Ensure that a minimal audit policy is created (Scored) Rationale Logging is an important detective control for all systems, to detect potential unauthorised access. Result: Does not pass. See the known issue for details. Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-policy-file is set. Review the contents of the file specified and ensure that it contains avalid audit policy. Remediation: Create an audit policy file for your cluster. 3.2.2 \u00b6 Ensure that the audit policy covers key security concerns (Not Scored) Rationale Security audit logs should cover access and modification of key resources in the cluster, to enable them to form an effective part of a security environment. Result: Not Scored - Operator Dependent Remediation: 4 Worker Node Security Configuration \u00b6 4.1 Worker Node Configuration Files \u00b6 4.1.1 \u00b6 Ensure that the kubelet service file permissions are set to 644 or more restrictive (Scored) Rationale The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Remediation: RKE2 doesn\u2019t launch the kubelet as a service. It is launched and managed by the RKE2 supervisor process. All configuration is passed to it as command line arguments at run time. 4.1.2 \u00b6 Ensure that the kubelet service file ownership is set to root:root (Scored) Rationale The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root . Result: Not Applicable Remediation: RKE2 doesn\u2019t launch the kubelet as a service. It is launched and managed by the RKE2 supervisor process. All configuration is passed to it as command line arguments at run time. 4.1.3 \u00b6 Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Scored) Rationale The kube-proxy kubeconfig file controls various parameters of the kube-proxy service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run kube-proxy with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Result: Pass Audit: Run the below command on the worker node. stat -c %a /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy.yaml 644 Verify that if a file is specified and it exists, the permissions are 644 or more restrictive. Remediation: By derfault, RKE2 creates rke2-kube-proxy.yaml with 644 permissions. No manual remediation needed. 4.1.4 \u00b6 Ensure that the proxy kubeconfig file ownership is set to root:root (Scored) Rationale The kubeconfig file for kube-proxy controls various parameters for the kube-proxy service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root . Result: Pass Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy.yaml root:root Verify that if a file is specified and it exists, the permissions are 644 or more restrictive. Remediation: By default, RKE2 creates rke2-kube-proxy.yaml with root:root ownership. No manual remediation needed. 4.1.5 \u00b6 Ensure that the kubelet.conf file permissions are set to 644 or more restrictive (Scored) Rationale The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Audit: Run the below command on the worker node. stat -c %a /var/lib/rancher/rke2/agent/kubelet.kubeconfig 644 Remediation: By derfault, RKE2 creates kubelet.kubeconfig with 644 permissions. No manual remediation needed. 4.1.6 \u00b6 Ensure that the kubelet.conf file ownership is set to root:root (Scored) Rationale The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root . Result: Not Applicable Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/agent/kubelet.kubeconfig root:root Remediation: By default, RKE2 creates kubelet.kubeconfig with root:root ownership. No manual remediation needed. 4.1.7 \u00b6 Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Scored) Rationale The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: Run the below command on the master node. stat -c %a /var/lib/rancher/rke2/server/tls/server-ca.crt 644 Verify that the permissions are 644. Remediation: By default, RKE2 creates /var/lib/rancher/rke2/server/tls/server-ca.crt with 644 permissions. 4.1.8 \u00b6 Ensure that the client certificate authorities file ownership is set to root:root (Scored) Rationale The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root . Result: Pass Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/server/tls/client-ca.crt root:root Remediation: By default, RKE2 creates /var/lib/rancher/rke2/server/tls/client-ca.crt with root:root ownership. 4.1.9 \u00b6 Ensure that the kubelet configuration file has permissions set to 644 or more restrictive (Scored) Rationale The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Remediation: RKE2 doesn\u2019t require or maintain a configuration file for the kubelet process. All configuration is passed to it as command line arguments at run time. 4.1.10 \u00b6 Ensure that the kubelet configuration file ownership is set to root:root (Scored) Rationale The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by root:root . Result: Not Applicable Remediation: RKE2 doesn\u2019t require or maintain a configuration file for the kubelet process. All configuration is passed to it as command line arguments at run time. 4.2 Kubelet \u00b6 This section contains recommendations for kubelet configuration. 4.2.1 \u00b6 Ensure that the --anonymous-auth argument is set to false (Scored) Rationale When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the value for --anonymous-auth is false. Remediation: By default, RKE2 starts kubelet with --anonymous-auth set to false. No manual remediation needed. 4.2.2 \u00b6 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored) Rationale Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that AlwaysAllow is not present. Remediation: RKE2 starts kubelet with Webhook as the value for the --authorization-mode argument. No manual remediation needed. 4.2.3 \u00b6 Ensure that the --client-ca-file argument is set as appropriate (Scored) Rationale The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the --client-ca-file argument has a ca file associated. Remediation: By default, RKE2 starts the kubelet process with the --client-ca-file . No manual remediation needed. 4.2.4 \u00b6 Ensure that the --read-only-port argument is set to 0 (Scored) Rationale The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the --read-only-port argument is set to 0. Remediation: By default, RKE2 starts the kubelet process with the --read-only-port argument set to 0. 4.2.5 \u00b6 Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Scored) Rationale Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that there's nothing returned. Remediation: By default, RKE2 does not set --streaming-connection-idle-timeout when starting kubelet. 4.2.6 \u00b6 Ensure that the --protect-kernel-defaults argument is set to true (Scored) Rationale Kernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: When running with the profile flag set to cis-1.5 , RKE2 starts the kubelet process with the --protect-kernel-defaults argument set to true. 4.2.7 \u00b6 Ensure that the --make-iptables-util-chains argument is set to true (Scored) Rationale Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify there are no results returned. Remediation: By default, RKE2 does not set the --make-iptables-util-chains argument. No manual remediation needed. 4.2.8 \u00b6 Ensure that the --hostname-override argument is not set (Not Scored) Rationale Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Result: Not Applicable Remediation: RKE2 does set this parameter for each host, but RKE2 also manages all certificates in the cluster. It ensures the hostname-override is included as a subject alternative name (SAN) in the kubelet's certificate. 4.2.9 \u00b6 Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture (Not Scored) Rationale It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Result: Not Scored - Operator Dependent Remediation: See CIS Benchmark guide for further details on configuring this. 4.2.10 \u00b6 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Scored) Rationale Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify the --tls-cert-file and --tls-private-key-file arguments are present and set appropriately. Remediation: By default, RKE2 sets the --tls-cert-file and --tls-private-key-file arguments when executing the kubelet process. 4.2.11 \u00b6 Ensure that the --rotate-certificates argument is not set to false (Scored) Rationale The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Note: This feature also require the RotateKubeletClientCertificate feature gate to be enabled (which is the default since Kubernetes v1.7) Result: Not Applicable Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation. 4.2.12 \u00b6 Ensure that the RotateKubeletServerCertificate argument is set to true (Scored) Rationale RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Result: Not Applicable Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation. 4.2.13 \u00b6 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Not Scored) Rationale TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Result: Not Scored - Operator Dependent Remediation: Configuration of the parameter is dependent on your use case. Please see the CIS Kubernetes Benchmark for suggestions on configuring this for your usecase. 5 Kubernetes Policies \u00b6 5.1 RBAC and Service Accounts \u00b6 5.1.1 \u00b6 Ensure that the cluster-admin role is only used where required (Not Scored) Rationale Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding , it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding , it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Result: Pass Remediation: RKE2 does not make inappropriate use of the cluster-admin role. Operators must audit their workloads of additional usage. See the CIS Benchmark guide for more details. 5.1.2 \u00b6 Minimize access to secrets (Not Scored) Rationale Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Result: Not Scored - Operator Dependent Remediation: RKE2 limits its use of secrets for the system components appropriately, but operators must audit the use of secrets by their workloads. See the CIS Benchmark guide for more details. 5.1.3 \u00b6 Minimize wildcard use in Roles and ClusterRoles (Not Scored) Rationale The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API. Result: Not Scored - Operator Dependent Audit: Run the below command on the master node. # Retrieve the roles defined across each namespaces in the cluster and review for wildcards /var/lib/rancher/rke2/bin/kubectl get roles --all-namespaces -o yaml # Retrieve the cluster roles defined in the cluster and review for wildcards /var/lib/rancher/rke2/bin/kubectl get clusterroles -o yaml Verify that there are not wildcards in use. Remediation: Operators should review their workloads for proper role usage. See the CIS Benchmark guide for more details. 5.1.4 \u00b6 Minimize access to create pods (Not Scored) Rationale The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Result: Not Scored - Operator Dependent Remediation: Operators should review who has access to create pods in their cluster. See the CIS Benchmark guide for more details. 5.1.5 \u00b6 Ensure that default service accounts are not actively used. (Scored) Rationale Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. Result: Pass. Currently requires operator intervention See the known issue for details. Audit: For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account. Remediation: Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false 5.1.6 \u00b6 Ensure that Service Account Tokens are only mounted where necessary (Not Scored) Rationale Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Result: Not Scored - Operator Dependent Remediation: The pods launched by RKE2 are part of the control plane and generally need access to communicate with the API server, thus this control does not apply to them. Operators should review their workloads and take steps to modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. 5.2 Pod Security Policies \u00b6 5.2.1 \u00b6 Minimize the admission of containers wishing to share the host process ID namespace (Scored) Rationale Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one PodSecurityPolicy (PSP) defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl describe psp global-restricted-psp | grep MustRunAsNonRoot Verify that the result is Rule: MustRunAsNonRoot . Remediation: RKE2, when run with the --profile=cis-1.5 argument, will disallow the use of privileged containers. 5.2.2 \u00b6 Minimize the admission of containers wishing to share the host process ID namespace (Scored) Rationale A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostPID == null) or (.spec.hostPID == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the hostPID value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed. 5.2.3 \u00b6 Minimize the admission of containers wishing to share the host IPC namespace (Scored) Rationale A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host IPC namespace. If you have a requirement to containers which require hostIPC, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostIPC == null) or (.spec.hostIPC == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the HostIPC value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed. 5.2.4 \u00b6 Minimize the admission of containers wishing to share the host network namespace (Scored) Rationale A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host network namespace. If you have need to run containers which require hostNetwork, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostNetwork == null) or (.spec.hostNetwork == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the HostNetwork value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed. 5.2.5 \u00b6 Minimize the admission of containers with allowPrivilegeEscalation (Scored) Rationale A container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.allowPrivilegeEscalation == null) or (.spec.allowPrivilegeEscalation == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the allowPrivilegeEscalation value to false explicitly for the PSP it creates. No manual remediation is needed. 5.2.6 \u00b6 Minimize the admission of root containers (Not Scored) Rationale Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user. There should be at least one PodSecurityPolicy (PSP) defined which does not permit root users in a container. If you need to run root containers, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.allowPrivilegeEscalation == null) or (.spec.allowPrivilegeEscalation == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the runAsUser.Rule value to MustRunAsNonRoot in the PodSecurityPolicy that it creates. No manual remediation is needed. 5.2.7 \u00b6 Minimize the admission of containers with the NET_RAW capability (Not Scored) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability. There should be at least one PodSecurityPolicy (PSP) defined which prevents containers with the NET_RAW capability from launching. If you need to run containers with this capability, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp global-restricted-psp -o json | jq .spec.requiredDropCapabilities [] Verify the value is \"ALL\" . Remediation: RKE2 sets .spec.requiredDropCapabilities[] to a value of All . No manual remediation needed. 5.2.8 \u00b6 Minimize the admission of containers with added capabilities (Not Scored) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks. There should be at least one PodSecurityPolicy (PSP) defined which prevents containers with capabilities beyond the default set from launching. If you need to run containers with additional capabilities, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Not Scored Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp Verify that there are no PSPs present which have allowedCapabilities set to anything other than an empty array. Remediation: When run with the --profile=cis-1.5 argument RKE2 applies a PodSecurityPolicy that sets requiredDropCapabilities to ALL . No manual remediation needed. 5.2.9 \u00b6 Minimize the admission of containers with capabilities assigned (Not Scored) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized. Result: Not Scored Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp Remediation: When run with the --profile=cis-1.5 argument RKE2 applies a PodSecurityPolicy that sets requiredDropCapabilities to ALL . No manual remediation needed. 5.3 Network Policies and CNI \u00b6 5.3.1 \u00b6 Ensure that the CNI in use supports Network Policies (Not Scored) Rationale Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Result: Pass Audit: Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies. Remediation: By default, RKE2 use Canal (Calico and Flannel) and fully supports network policies. 5.3.2 \u00b6 Ensure that all Namespaces have Network Policies defined (Scored) Rationale Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Result: Pass Audit: Run the below command on the master node. for i in kube-system kube-public default ; do /var/lib/rancher/rke2/bin/kubectl get networkpolicies -n $i ; done Verify that there are network policies applied to each of the namespaces. Remediation: RKE2, when executed with the --profile=cis-1.5 argument applies a secure network policy that only allows intra-namespace traffic and DNS to kube-system. No manual remediation needed. 5.4 Secrets Management \u00b6 5.4.1 \u00b6 Prefer using secrets as files over secrets as environment variables (Not Scored) Rationale It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Result: Not Scored Audit: Run the following command to find references to objects which use environment variables defined from secrets. /var/lib/rancher/rke2/bin/kubectl get all -o jsonpath = '{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {\"\\n\"}{end}' -A Remediation: If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. 5.4.2 \u00b6 Consider external secret storage (Not Scored) Rationale Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Result: Not Scored Audit: Review your secrets management implementation. Remediation: Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. 5.5 Extensible Admission Control \u00b6 5.5.1 \u00b6 Configure Image Provenance using ImagePolicyWebhook admission controller (Not Scored) Rationale Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster. Result: Not Scored Audit: Review the pod definitions in your cluster and verify that image provenance is configured as appropriate. Remediation: Follow the Kubernetes documentation and setup image provenance. 5.6 Omitted \u00b6 The v1.5.1 guide skips 5.6 and goes from 5.5 to 5.7. We are including it here merely for explanation. 5.7 General Policies \u00b6 These policies relate to general cluster management topics, like namespace best practices and policies applied to pod objects in the cluster. 5.7.1 \u00b6 Create administrative boundaries between resources using namespaces (Not Scored) Rationale Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Result: Not Scored Audit: Run the below command and review the namespaces created in the cluster. /var/lib/rancher/rke2/bin/kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements. Remediation: Follow the documentation and create namespaces for objects in your deployment as you need them. 5.7.2 \u00b6 Ensure that the seccomp profile is set to docker/default in your pod definitions (Not Scored) Rationale Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container. Result: Not Scored Audit: Review the pod definitions in your cluster. It should create a line as below: annotations : seccomp.security.alpha.kubernetes.io/pod : docker/default Remediation: Review the Kubernetes documentation and if needed, apply a relevant PodSecurityPolicy. 5.7.3 \u00b6 Apply Security Context to Your Pods and Containers (Not Scored) Rationale A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Result: Not Scored Audit: Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate. Remediation: Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark. 5.7.4 \u00b6 The default namespace should not be used (Scored) Rationale Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get all -n default Verify that there are no resources applied to the default namespace. Remediation: By default, RKE2 does not utilize the default namespace.","title":"CIS Self-Assessment Guide"},{"location":"security/cis_self_assessment/#cis-kubernetes-benchmark-v15-rke2-v118","text":"","title":"CIS Kubernetes Benchmark v1.5 - RKE2 v1.18"},{"location":"security/cis_self_assessment/#overview","text":"This document is a companion to the RKE2 security hardening guide. The hardening guide provides prescriptive guidance for hardening a production installation of RKE2, and this benchmark guide is meant to help you evaluate the level of security of the hardened cluster against each control in the CIS Kubernetes benchmark. It is to be used by RKE2 operators, security teams, auditors, and decision makers. This guide is specific to the v1.18 release line of RKE2 and the v1.5.1 release of the CIS Kubernetes Benchmark. For more detail about each control, including more detailed descriptions and remediations for failing tests, you can refer to the corresponding section of the CIS Kubernetes Benchmark v1.5. You can download the benchmark after logging in to CISecurity.org .","title":"Overview"},{"location":"security/cis_self_assessment/#testing-controls-methodology","text":"Each control in the CIS Kubernetes Benchmark was evaluated against an RKE2 cluster that was configured according to the accompanying hardening guide. Where control audits differ from the original CIS benchmark, the audit commands specific to RKE2 are provided for testing. These are the possible results for each control: Pass - The RKE2 cluster under test passed the audit outlined in the benchmark. Not Applicable - The control is not applicable to RKE2 because of how it is designed to operate. The remediation section will explain why this is so. Not Scored - Operator Dependent - The control is not scored in the CIS benchmark and it depends on the cluster's use case or some other factor that must be determined by the cluster operator. These controls have been evaluated to ensure RKE2 does not prevent their implementation, but no further configuration or auditing of the cluster under test has been performed.","title":"Testing controls methodology"},{"location":"security/cis_self_assessment/#controls","text":"","title":"Controls"},{"location":"security/cis_self_assessment/#1-master-node-security-configuration","text":"","title":"1 Master Node Security Configuration"},{"location":"security/cis_self_assessment/#11-master-node-configuration-files","text":"","title":"1.1 Master Node Configuration Files"},{"location":"security/cis_self_assessment/#111","text":"Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Scored) Rationale The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-apiserver.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed.","title":"1.1.1"},{"location":"security/cis_self_assessment/#112","text":"Ensure that the API server pod specification file ownership is set to root:root (Scored) Rationale The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root . Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-apiserver.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed.","title":"1.1.2"},{"location":"security/cis_self_assessment/#113","text":"Ensure that the controller manager pod specification file permissions are set to 644 or more restrictive (Scored) Rationale The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-controller-manager.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed.","title":"1.1.3"},{"location":"security/cis_self_assessment/#114","text":"Ensure that the controller manager pod specification file ownership is set to root:root (Scored) Rationale The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-controller-manager.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed.","title":"1.1.4"},{"location":"security/cis_self_assessment/#115","text":"Ensure that the scheduler pod specification file permissions are set to 644 or more restrictive (Scored) Rationale The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed.","title":"1.1.5"},{"location":"security/cis_self_assessment/#116","text":"Ensure that the scheduler pod specification file ownership is set to root:root (Scored) Rationale The scheduler pod specification file controls various parameters that set the behavior of the kube-scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed.","title":"1.1.6"},{"location":"security/cis_self_assessment/#117","text":"Ensure that the etcd pod specification file permissions are set to 644 or more restrictive (Scored) Rationale The etcd pod specification file /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly- available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml 644 Remediation: By default, RKE2 creates these files with 644 permissions. No manual remediation needed.","title":"1.1.7"},{"location":"security/cis_self_assessment/#118","text":"Ensure that the etcd pod specification file ownership is set to root:root (Scored) Rationale The etcd pod specification file /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly- available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/agent/pod-manifests/etcd.yaml root:root Remediation: By default, RKE2 creates these files with root:root ownership. No manual remediation needed.","title":"1.1.8"},{"location":"security/cis_self_assessment/#119","text":"Ensure that the Container Network Interface file permissions are set to 644 or more restrictive (Not Scored) Rationale Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/manifests/rke2-canal.yml 644 Remediation: RKE2 deploys the default CNI, Canal, using a Helm chart. The chart is defined as a custom resource in a file with 644 permissions. No manual remediation needed.","title":"1.1.9"},{"location":"security/cis_self_assessment/#1110","text":"Ensure that the Container Network Interface file ownership is set to root:root (Not Scored) Rationale Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/manifests/rke2-canal.yml root:root Remediation: RKE2 deploys the default CNI, Canal, using a Helm chart. The chart is defined as a custom resource in a file with root:root ownership. No manual remediation needed.","title":"1.1.10"},{"location":"security/cis_self_assessment/#1111","text":"Ensure that the etcd data directory permissions are set to 700 or more restrictive (Scored) Rationale etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world. Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/db/etcd 700 Remediation: RKE2 manages the etcd data directory and sets its permissions to 700. No manual remediation needed.","title":"1.1.11"},{"location":"security/cis_self_assessment/#1112","text":"Ensure that the etcd data directory ownership is set to etcd:etcd (Scored) Rationale etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/db/etcd etcd:etcd Remediation: When running RKE2 with the profile flag set to cis-1.5 , RKE2 will refuse to start if the etcd user and group doesn't exist on the host. If it does exist, RKE2 will automatically set the ownership of the etcd data directory to etcd:etcd and ensure the etcd static pod is started with that user and group.","title":"1.1.12"},{"location":"security/cis_self_assessment/#1113","text":"Ensure that the admin.conf file permissions are set to 644 or more restrictive (Scored) Rationale The admin.conf is the administrator kubeconfig file defining various settings for the administration of the cluster. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/admin.kubeconfig . Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/admin.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/admin.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed.","title":"1.1.13"},{"location":"security/cis_self_assessment/#1114","text":"Ensure that the admin.conf file ownership is set to root:root (Scored) Rationale The admin.conf file contains the admin credentials for the cluster. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/admin.kubeconfig . Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/admin.kubeconfig root:root Remediation: By default, RKE2 creates this file at stat -c %U:%G /var/lib/rancher/rke2/server/cred/admin.kubeconfig and automatically sets its ownership to root:root .","title":"1.1.14"},{"location":"security/cis_self_assessment/#1115","text":"Ensure that the scheduler.conf file permissions are set to 644 or more restrictive (Scored) Rationale The scheduler.conf file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig . Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed.","title":"1.1.15"},{"location":"security/cis_self_assessment/#1116","text":"Ensure that the scheduler.conf file ownership is set to root:root (Scored) Rationale The scheduler.conf file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig . Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig root:root Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig and automatically sets its ownership to root:root .","title":"1.1.16"},{"location":"security/cis_self_assessment/#1117","text":"Ensure that the controller.kubeconfig file permissions are set to 644 or more restrictive (Scored) Rationale The controller.kubeconfig file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/controller.kubeconfig . Result: Pass Audit: stat -c %a /var/lib/rancher/rke2/server/cred/controller.kubeconfig 644 Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/controller.kubeconfig and automatically sets its permissions to 644 . No manual remediation needed.","title":"1.1.17"},{"location":"security/cis_self_assessment/#1118","text":"Ensure that the controller.kubeconfig file ownership is set to root:root (Scored) Rationale The controller.kubeconfig file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. In RKE2, this file is located at /var/lib/rancher/rke2/server/cred/controller.kubeconfig . Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/cred/controller.kubeconfig root:root Remediation: By default, RKE2 creates this file at /var/lib/rancher/rke2/server/cred/controller.kubeconfig and automatically sets its ownership to root:root .","title":"1.1.18"},{"location":"security/cis_self_assessment/#1119","text":"Ensure that the Kubernetes PKI directory and file ownership is set to root:root (Scored) Rationale Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root. Result: Pass Audit: stat -c %U:%G /var/lib/rancher/rke2/server/tls root:root Remediation: By default, RKE2 creates the directory and files with the expected ownership of root:root . No manual remediation should be necessary.","title":"1.1.19"},{"location":"security/cis_self_assessment/#1120","text":"Ensure that the Kubernetes PKI certificate file permissions are set to 644 or more restrictive (Scored) Rationale Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 644 or more restrictive to protect their integrity. Result: Pass Audit: Run the below command on the master node. stat -c %n \\ %a /var/lib/rancher/rke2/server/tls/*.crt Verify that the permissions are 644 or more restrictive. Remediation: By default, RKE2 creates the files with the expected permissions of 644 . No manual remediation is needed.","title":"1.1.20"},{"location":"security/cis_self_assessment/#1121","text":"Ensure that the Kubernetes PKI key file permissions are set to 600 (Scored) Rationale Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality. Result: Pass Audit: Run the below command on the master node. stat -c %n \\ %a /var/lib/rancher/rke2/server/tls/*.key Verify that the permissions are 600 or more restrictive. Remediation: By default, RKE2 creates the files with the expected permissions of 600 . No manual remediation is needed.","title":"1.1.21"},{"location":"security/cis_self_assessment/#12-api-server","text":"This section contains recommendations relating to API server configuration flags","title":"1.2 API Server"},{"location":"security/cis_self_assessment/#121","text":"Ensure that the --anonymous-auth argument is set to false (Not Scored) Rationale When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that --anonymous-auth=false is present. Remediation: By default, RKE2 kube-apiserver is configured to run with this flag and value. No manual remediation is needed.","title":"1.2.1"},{"location":"security/cis_self_assessment/#122","text":"Ensure that the --basic-auth-file argument is not set (Scored) Rationale Basic authentication uses plaintext credentials for authentication. Currently, the basic authentication credentials last indefinitely, and the password cannot be changed without restarting the API server. The basic authentication is currently supported for convenience. Hence, basic authentication should not be used. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --basic-auth-file argument does not exist. Remediation: By default, RKE2 does not run with basic authentication enabled. No manual remediation is needed.","title":"1.2.2"},{"location":"security/cis_self_assessment/#123","text":"Ensure that the --token-auth-file parameter is not set (Scored) Rationale The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token-based authentication. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --basic-auth-file argument does not exist. Remediation: By default, RKE2 does not run with basic authentication enabled. No manual remediation is needed.","title":"1.2.3"},{"location":"security/cis_self_assessment/#124","text":"Ensure that the --kubelet-https argument is set to true (Scored) Rationale Connections from apiserver to kubelets could potentially carry sensitive data such as secrets and keys. It is thus important to use in-transit encryption for any communication between the apiserver and kubelets. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-https argument does not exist. Remediation: By default, RKE2 kube-apiserver doesn't run with the --kubelet-https parameter as it runs with TLS. No manual remediation is needed.","title":"1.2.4"},{"location":"security/cis_self_assessment/#125","text":"Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate (Scored) Rationale The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate. Remediation: By default, RKE2 kube-apiserver is ran with these arguments for secure communication with kubelet. No manual remediation is needed.","title":"1.2.5"},{"location":"security/cis_self_assessment/#126","text":"Ensure that the --kubelet-certificate-authority argument is set as appropriate (Scored) Rationale The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --kubelet-certificate-authority argument exists and is set as appropriate. Remediation: By default, RKE2 kube-apiserver is ran with this argument for secure communication with kubelet. No manual remediation is needed.","title":"1.2.6"},{"location":"security/cis_self_assessment/#127","text":"Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored) Rationale The API Server, can be configured to allow all requests. This mode should not be used on any production cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the argument value doesn't contain AlwaysAllow . Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed.","title":"1.2.7"},{"location":"security/cis_self_assessment/#128","text":"Ensure that the --authorization-mode argument includes Node (Scored) Rationale The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify Node exists as a parameter to the argument. Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed.","title":"1.2.8"},{"location":"security/cis_self_assessment/#129","text":"Ensure that the --authorization-mode argument includes RBAC (Scored) Rationale Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify RBAC exists as a parameter to the argument. Remediation: By default, RKE2 sets Node,RBAC as the parameter to the --authorization-mode argument. No manual remediation is needed.","title":"1.2.9"},{"location":"security/cis_self_assessment/#1210","text":"Ensure that the admission control plugin EventRateLimit is set (Not Scored) Rationale Using EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept. Note: This is an Alpha feature in the Kubernetes 1.15 release. Result: Not Scored - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes EventRateLimit. Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. To configure this, follow the Kubernetes documentation and set the desired limits in a configuration file. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter.","title":"1.2.10"},{"location":"security/cis_self_assessment/#1211","text":"Ensure that the admission control plugin AlwaysAdmit is not set (Scored) Rationale Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that if the --enable-admission-plugins argument is set, its value does not include AlwaysAdmit . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed.","title":"1.2.11"},{"location":"security/cis_self_assessment/#1212","text":"Ensure that the admission control plugin AlwaysPullImages is set (Not Scored) Rationale Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image\u2019s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. Result: Not Scored - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes AlwaysPullImages . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. To configure this, follow the Kubernetes documentation and set the desired limits in a configuration file. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter.","title":"1.2.12"},{"location":"security/cis_self_assessment/#1213","text":"Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used (Not Scored) Rationale SecurityContextDeny can be used to provide a layer of security for clusters which do not have PodSecurityPolicies enabled. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes SecurityContextDeny , if PodSecurityPolicy is not included. Remediation: By default, RKE2 automatically enables the PodSecurityPolicy admission plugin. Therefore, the SecurityContextDeny plugin need not be enabled. No manual remediation needed.","title":"1.2.13"},{"location":"security/cis_self_assessment/#1214","text":"Ensure that the admission control plugin ServiceAccount is set (Scored) Rationale When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --disable-admission-plugins argument is set to a value that does not includes ServiceAccount . Remediation: By default, RKE2 does not use this argument. If there's a desire to use this argument, follow the documentation and create ServiceAccount objects as per your environment. Then refer to RKE2's documentation to see how to supply additional api server configuration via the kube-apiserver-arg parameter.","title":"1.2.14"},{"location":"security/cis_self_assessment/#1215","text":"Ensure that the admission control plugin NamespaceLifecycle is set (Scored) Rationale Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --disable-admission-plugins argument is set to a value that does not include NamespaceLifecycle . Remediation: By default, RKE2 does not use this argument. No manual remediation needed.","title":"1.2.15"},{"location":"security/cis_self_assessment/#1216","text":"Ensure that the admission control plugin PodSecurityPolicy is set (Scored) Rationale A Pod Security Policy is a cluster-level resource that controls the actions that a pod can perform and what it has the ability to access. The PodSecurityPolicy objects define a set of conditions that a pod must run with in order to be accepted into the system. Pod Security Policies are comprised of settings and strategies that control the security features a pod has access to and hence this must be used to control pod access permissions. Note: When the PodSecurityPolicy admission plugin is in use, there needs to be at least one PodSecurityPolicy in place for ANY pods to be admitted. See section 1.7 for recommendations on PodSecurityPolicy settings. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --enable-admission-plugins argument is set to a value that includes PodSecurityPolicy . Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed.","title":"1.2.16"},{"location":"security/cis_self_assessment/#1217","text":"Ensure that the admission control plugin NodeRestriction is set (Scored) Rationale Using the NodeRestriction plug-in ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Remediation: By default, RKE2 only sets NodeRestriction,PodSecurityPolicy as the parameter to the --enable-admission-plugins argument. No manual remediation needed.","title":"1.2.17"},{"location":"security/cis_self_assessment/#1218","text":"Ensure that the --insecure-bind-address argument is not set (Scored) Rationale If you bind the apiserver to an insecure address, basically anyone who could connect to it over the insecure port, would have unauthenticated and unencrypted access to your master node. The apiserver doesn't do any authentication checking for insecure binds and traffic to the Insecure API port is not encrpyted, allowing attackers to potentially read sensitive data in transit. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --insecure-bind-address argument does not exist. Remediation: By default, RKE2 explicitly excludes the use of the --insecure-bind-address parameter. No manual remediation is needed.","title":"1.2.18"},{"location":"security/cis_self_assessment/#1219","text":"Ensure that the --insecure-port argument is set to 0 (Scored) Rationale Setting up the apiserver to serve on an insecure port would allow unauthenticated and unencrypted access to your master node. This would allow attackers who could access this port, to easily take control of the cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --insecure-port argument is set to 0. Remediation: By default, RKE2 starts the kube-apiserver process with this argument's parameter set to 0. No manual remediation is needed.","title":"1.2.19"},{"location":"security/cis_self_assessment/#1220","text":"Ensure that the --secure-port argument is not set to 0 (Scored) Rationale The secure port is used to serve https with authentication and authorization. If you disable it, no https traffic is served and all traffic is served unencrypted. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --secure-port argument is either not set or is set to an integer value between 1 and 65535. Remediation: By default, RKE2 sets the parameter of 6443 for the --secure-port argument. No manual remediation is needed.","title":"1.2.20"},{"location":"security/cis_self_assessment/#1221","text":"Ensure that the --profiling argument is set to false (Scored) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed.","title":"1.2.21"},{"location":"security/cis_self_assessment/#1222","text":"Ensure that the --audit-log-path argument is set (Scored) Rationale Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-path argument is set as appropriate. Remediation: By default, RKE2 sets the --audit-log-path argument and parameter. No manual remediation needed.","title":"1.2.22"},{"location":"security/cis_self_assessment/#1223","text":"Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Scored) Rationale Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxage argument is set to 30 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxage argument parameter to 30. No manual remediation needed.","title":"1.2.23"},{"location":"security/cis_self_assessment/#1224","text":"Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Scored) Rationale Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxbackup argument parameter to 10. No manual remediation needed.","title":"1.2.24"},{"location":"security/cis_self_assessment/#1225","text":"Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Scored) Rationale Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-log-maxsize argument is set to 100 or as appropriate. Remediation: By default, RKE2 sets the --audit-log-maxsize argument parameter to 100. No manual remediation needed.","title":"1.2.25"},{"location":"security/cis_self_assessment/#1226","text":"Ensure that the --request-timeout argument is set as appropriate (Scored) Rationale Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --request-timeout argument is either not set or set to an appropriate value. Remediation: By default, RKE2 does not set the --request-timeout argument. No manual remediation needed.","title":"1.2.26"},{"location":"security/cis_self_assessment/#1227","text":"Ensure that the --service-account-lookup argument is set to true (Scored) Rationale If --service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that if the --service-account-lookup argument exists it is set to true. Remediation: By default, RKE2 doesn't set this argument in favor of taking the default effect. No manual remediation needed.","title":"1.2.27"},{"location":"security/cis_self_assessment/#1228","text":"Ensure that the --service-account-key-file argument is set as appropriate (Scored) Rationale By default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file . Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --service-account-key-file argument exists and is set as appropriate. Remediation: By default, RKE2 sets the --service-account-key-file explicitly. No manual remediation needed.","title":"1.2.28"},{"location":"security/cis_self_assessment/#1229","text":"Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate. Remediation: By default, RKE2 sets the --etcd-certfile and --etcd-keyfile arguments explicitly. No manual remediation needed.","title":"1.2.29"},{"location":"security/cis_self_assessment/#1230","text":"Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Scored) Rationale API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate. Remediation: By default, RKE2 sets the --tls-cert-file and --tls-private-key-file arguments explicitly. No manual remediation needed.","title":"1.2.30"},{"location":"security/cis_self_assessment/#1231","text":"Ensure that the --client-ca-file argument is set as appropriate (Scored) Rationale API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If --client-ca-file argument is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --client-ca-file argument exists and it is set as appropriate. Remediation: By default, RKE2 sets the --client-ca-file argument explicitly. No manual remediation needed.","title":"1.2.31"},{"location":"security/cis_self_assessment/#1232","text":"Ensure that the --etcd-cafile argument is set as appropriate (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --etcd-cafile argument exists and it is set as appropriate. Remediation: By default, RKE2 sets the --etcd-cafile argument explicitly. No manual remediation needed.","title":"1.2.32"},{"location":"security/cis_self_assessment/#1233","text":"Ensure that the --encryption-provider-config argument is set as appropriate (Scored) Rationale etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --encryption-provider-config argument is set to a EncryptionConfigfile. Additionally, ensure that the EncryptionConfigfile has all the desired resources covered especially any secrets. Remediation: By default, RKE2 sets the --encryption-provider-config argument explicitly. No manual remediation needed. RKE2's default encryption provider config file is located at /var/lib/rancher/rke2/server/cred/encryption-config.json and is configured to encrypt secrets.","title":"1.2.33"},{"location":"security/cis_self_assessment/#1234","text":"Ensure that encryption providers are appropriately configured (Scored) Rationale Where etcd encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the aescbc , kms and secretbox are likely to be appropriate options. Result: Pass Remediation: Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc , kms or secretbox as the encryption provider. Audit: Run the below command on the master node. grep aescbc /var/lib/rancher/rke2/server/cred/encryption-config.json Run the below command on the master node. Verify that aescbc is set as the encryption provider for all the desired resources. Remediation By default, RKE2 sets the argument --encryption-provider-config and parameter. The contents of the config file indicates the use of aescbc. No manual remediation needed.","title":"1.2.34"},{"location":"security/cis_self_assessment/#1235","text":"Ensure that the API Server only makes use of Strong Cryptographic Ciphers (Not Scored) Rationale TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Result: Not Scored - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --tls-cipher-suites argument is set as outlined in the remediation procedure below. Remediation: By default, RKE2 explicitly doesn't set this flag. No manual remediation needed.","title":"1.2.35"},{"location":"security/cis_self_assessment/#13-controller-manager","text":"","title":"1.3 Controller Manager"},{"location":"security/cis_self_assessment/#131","text":"Ensure that the --terminated-pod-gc-threshold argument is set as appropriate (Not Scored) Rationale Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection. Result: Not Scored - Operator Dependent Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --terminated-pod-gc-threshold argument is set as appropriate. Remediation: By default, RKE2 sets the --terminated-pod-gc-threshold argument with a value of 1000. No manual remediation needed.","title":"1.3.1"},{"location":"security/cis_self_assessment/#132","text":"Ensure that the --profiling argument is set to false (Scored) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed.","title":"1.3.2"},{"location":"security/cis_self_assessment/#133","text":"Ensure that the --use-service-account-credentials argument is set to true (Scored) Rationale The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-service-account-credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --use-service-account-credentials argument is set to true. Remediation: By default, RKE2 sets the --use-service-account-credentials argument to true. No manual remediation needed.","title":"1.3.3"},{"location":"security/cis_self_assessment/#134","text":"Ensure that the --service-account-private-key-file argument is set as appropriate (Scored) Rationale To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private-key-file as appropriate. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --service-account-private-key-file argument is set as appropriate. Remediation: By default, RKE2 sets the --service-account-private-key-file argument with the service account key file. No manual remediation needed.","title":"1.3.4"},{"location":"security/cis_self_assessment/#135","text":"Ensure that the --root-ca-file argument is set as appropriate (Scored) Rationale Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate Remediation: By default, RKE2 sets the --root-ca-file argument with the root ca file. No manual remediation needed.","title":"1.3.5"},{"location":"security/cis_self_assessment/#136","text":"Ensure that the RotateKubeletServerCertificate argument is set to true (Scored) Rationale RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Result: Not Applicable Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that RotateKubeletServerCertificateargument exists and is set to true. Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation.","title":"1.3.6"},{"location":"security/cis_self_assessment/#137","text":"Ensure that the --bind-address argument is set to 127.0.0.1 (Scored) Rationale The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-controller-manager | grep -v grep Verify that the --bind-address argument is set to 127.0.0.1. Remediation: By default, RKE2 sets the --bind-address argument to 127.0.0.1 . No manual remediation needed.","title":"1.3.7"},{"location":"security/cis_self_assessment/#14-scheduler","text":"This section contains recommendations relating to Scheduler configuration flags","title":"1.4 Scheduler"},{"location":"security/cis_self_assessment/#141","text":"Ensure that the --profiling argument is set to false (Scored) Rationale Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-scheduler | grep -v grep Verify that the --profiling argument is set to false. Remediation: By default, RKE2 sets the --profiling flag parameter to false. No manual remediation needed.","title":"1.4.1"},{"location":"security/cis_self_assessment/#142","text":"Ensure that the --bind-address argument is set to 127.0.0.1 (Scored) Rationale The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kube-scheduler | grep -v grep Verify that the --bind-address argument is set to 127.0.0.1. Remediation: By default, RKE2 sets the --bind-address argument to 127.0.0.1 . No manual remediation needed.","title":"1.4.2"},{"location":"security/cis_self_assessment/#2-etcd-node-configuration","text":"This section covers recommendations for etcd configuration.","title":"2 Etcd Node Configuration"},{"location":"security/cis_self_assessment/#21","text":"Ensure that the cert-file and key-file fields are set as appropriate (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Result: Pass Audit: Run the below command on the master node. grep -E 'cert-file|key-file' /var/lib/rancher/rke2/server/db/etcd/config Verify that the cert-file and the key-file fields are set as appropriate. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Server and peer cert and key files are specified. No manual remediation needed.","title":"2.1"},{"location":"security/cis_self_assessment/#22","text":"Ensure that the client-cert-auth field is set to true (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Result: Pass Audit: Run the below command on the master node. grep 'client-cert-auth' /var/lib/rancher/rke2/server/db/etcd/config Verify that the client-cert-auth field is set to true. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . client-cert-auth is set to true. No manual remediation needed.","title":"2.2"},{"location":"security/cis_self_assessment/#23","text":"Ensure that the auto-tls field is not set to true (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Result: Pass Audit: Run the below command on the master node. grep 'auto-tls' /var/lib/rancher/rke2/server/db/etcd/config Verify that if the auto-tls field does not exist. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, it does not contain the auto-tls argument. No manual remediation needed.","title":"2.3"},{"location":"security/cis_self_assessment/#24","text":"Ensure that the peer-cert-file and peer-key-file fields are set as appropriate (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters. Result: Pass Audit: Run the below command on the master node. grep -E 'peer-server-client.crt|peer-server-client.key' /var/lib/rancher/rke2/server/db/etcd/config Verify that the peer-server-client.crt and peer-server-client.key fields are set as appropriate. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, the peer-server-client.crt and peer-server-client.key fields are set. No manual remediation needed.","title":"2.4"},{"location":"security/cis_self_assessment/#25","text":"Ensure that the client-cert-auth field is set to true (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Result: Not Applicable Audit: Run the below command on the master node. grep 'client-cert-auth' /var/lib/rancher/rke2/server/db/etcd/config Verify that the client-cert-auth field in the peer section is set to true. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, the client-cert-auth field is set. No manual remediation needed.","title":"2.5"},{"location":"security/cis_self_assessment/#26","text":"Ensure that the peer-auto-tls field is not set to true (Scored) Rationale etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self- signed certificates for authentication. Result: Pass Audit: Run the below command on the master node. grep 'peer-auto-tls' /var/lib/rancher/rke2/server/db/etcd/config Verify that if the peer-auto-tls field does not exist. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config . Within the file, it does not contain the peer-auto-tls field. No manual remediation needed.","title":"2.6"},{"location":"security/cis_self_assessment/#27","text":"Ensure that a unique Certificate Authority is used for etcd (Not Scored) Rationale etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database. Result: Pass Audit: Run the below command on the master node. # To find the ca file used by etcd: grep 'trusted-ca-file' /var/lib/rancher/rke2/server/db/etcd/config # To find the kube-apiserver process: /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the file referenced by the client-ca-file flag in the apiserver process is different from the file referenced by the trusted-ca-file parameter in the etcd configuration file. Remediation: By default, RKE2 uses a config file for etcd that can be found at /var/lib/rancher/rke2/server/db/etcd/config and the trusted-ca-file parameters in it are set to unique values specific to etcd. No manual remediation needed.","title":"2.7"},{"location":"security/cis_self_assessment/#3-control-plane-configuration","text":"","title":"3 Control Plane Configuration"},{"location":"security/cis_self_assessment/#31-authentication-and-authorization","text":"","title":"3.1 Authentication and Authorization"},{"location":"security/cis_self_assessment/#311","text":"Client certificate authentication should not be used for users (Not Scored) Rationale With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Result: Not Scored - Operator Dependent Audit: Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication. Remediation: Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates.","title":"3.1.1"},{"location":"security/cis_self_assessment/#32-logging","text":"","title":"3.2 Logging"},{"location":"security/cis_self_assessment/#321","text":"Ensure that a minimal audit policy is created (Scored) Rationale Logging is an important detective control for all systems, to detect potential unauthorised access. Result: Does not pass. See the known issue for details. Audit: Run the below command on the master node. /bin/ps -ef | grep kube-apiserver | grep -v grep Verify that the --audit-policy-file is set. Review the contents of the file specified and ensure that it contains avalid audit policy. Remediation: Create an audit policy file for your cluster.","title":"3.2.1"},{"location":"security/cis_self_assessment/#322","text":"Ensure that the audit policy covers key security concerns (Not Scored) Rationale Security audit logs should cover access and modification of key resources in the cluster, to enable them to form an effective part of a security environment. Result: Not Scored - Operator Dependent Remediation:","title":"3.2.2"},{"location":"security/cis_self_assessment/#4-worker-node-security-configuration","text":"","title":"4 Worker Node Security Configuration"},{"location":"security/cis_self_assessment/#41-worker-node-configuration-files","text":"","title":"4.1 Worker Node Configuration Files"},{"location":"security/cis_self_assessment/#411","text":"Ensure that the kubelet service file permissions are set to 644 or more restrictive (Scored) Rationale The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Remediation: RKE2 doesn\u2019t launch the kubelet as a service. It is launched and managed by the RKE2 supervisor process. All configuration is passed to it as command line arguments at run time.","title":"4.1.1"},{"location":"security/cis_self_assessment/#412","text":"Ensure that the kubelet service file ownership is set to root:root (Scored) Rationale The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root . Result: Not Applicable Remediation: RKE2 doesn\u2019t launch the kubelet as a service. It is launched and managed by the RKE2 supervisor process. All configuration is passed to it as command line arguments at run time.","title":"4.1.2"},{"location":"security/cis_self_assessment/#413","text":"Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Scored) Rationale The kube-proxy kubeconfig file controls various parameters of the kube-proxy service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run kube-proxy with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Result: Pass Audit: Run the below command on the worker node. stat -c %a /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy.yaml 644 Verify that if a file is specified and it exists, the permissions are 644 or more restrictive. Remediation: By derfault, RKE2 creates rke2-kube-proxy.yaml with 644 permissions. No manual remediation needed.","title":"4.1.3"},{"location":"security/cis_self_assessment/#414","text":"Ensure that the proxy kubeconfig file ownership is set to root:root (Scored) Rationale The kubeconfig file for kube-proxy controls various parameters for the kube-proxy service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root . Result: Pass Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy.yaml root:root Verify that if a file is specified and it exists, the permissions are 644 or more restrictive. Remediation: By default, RKE2 creates rke2-kube-proxy.yaml with root:root ownership. No manual remediation needed.","title":"4.1.4"},{"location":"security/cis_self_assessment/#415","text":"Ensure that the kubelet.conf file permissions are set to 644 or more restrictive (Scored) Rationale The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Audit: Run the below command on the worker node. stat -c %a /var/lib/rancher/rke2/agent/kubelet.kubeconfig 644 Remediation: By derfault, RKE2 creates kubelet.kubeconfig with 644 permissions. No manual remediation needed.","title":"4.1.5"},{"location":"security/cis_self_assessment/#416","text":"Ensure that the kubelet.conf file ownership is set to root:root (Scored) Rationale The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root . Result: Not Applicable Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/agent/kubelet.kubeconfig root:root Remediation: By default, RKE2 creates kubelet.kubeconfig with root:root ownership. No manual remediation needed.","title":"4.1.6"},{"location":"security/cis_self_assessment/#417","text":"Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Scored) Rationale The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Pass Audit: Run the below command on the master node. stat -c %a /var/lib/rancher/rke2/server/tls/server-ca.crt 644 Verify that the permissions are 644. Remediation: By default, RKE2 creates /var/lib/rancher/rke2/server/tls/server-ca.crt with 644 permissions.","title":"4.1.7"},{"location":"security/cis_self_assessment/#418","text":"Ensure that the client certificate authorities file ownership is set to root:root (Scored) Rationale The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root . Result: Pass Audit: Run the below command on the master node. stat -c %U:%G /var/lib/rancher/rke2/server/tls/client-ca.crt root:root Remediation: By default, RKE2 creates /var/lib/rancher/rke2/server/tls/client-ca.crt with root:root ownership.","title":"4.1.8"},{"location":"security/cis_self_assessment/#419","text":"Ensure that the kubelet configuration file has permissions set to 644 or more restrictive (Scored) Rationale The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Result: Not Applicable Remediation: RKE2 doesn\u2019t require or maintain a configuration file for the kubelet process. All configuration is passed to it as command line arguments at run time.","title":"4.1.9"},{"location":"security/cis_self_assessment/#4110","text":"Ensure that the kubelet configuration file ownership is set to root:root (Scored) Rationale The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by root:root . Result: Not Applicable Remediation: RKE2 doesn\u2019t require or maintain a configuration file for the kubelet process. All configuration is passed to it as command line arguments at run time.","title":"4.1.10"},{"location":"security/cis_self_assessment/#42-kubelet","text":"This section contains recommendations for kubelet configuration.","title":"4.2 Kubelet"},{"location":"security/cis_self_assessment/#421","text":"Ensure that the --anonymous-auth argument is set to false (Scored) Rationale When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the value for --anonymous-auth is false. Remediation: By default, RKE2 starts kubelet with --anonymous-auth set to false. No manual remediation needed.","title":"4.2.1"},{"location":"security/cis_self_assessment/#422","text":"Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored) Rationale Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that AlwaysAllow is not present. Remediation: RKE2 starts kubelet with Webhook as the value for the --authorization-mode argument. No manual remediation needed.","title":"4.2.2"},{"location":"security/cis_self_assessment/#423","text":"Ensure that the --client-ca-file argument is set as appropriate (Scored) Rationale The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the --client-ca-file argument has a ca file associated. Remediation: By default, RKE2 starts the kubelet process with the --client-ca-file . No manual remediation needed.","title":"4.2.3"},{"location":"security/cis_self_assessment/#424","text":"Ensure that the --read-only-port argument is set to 0 (Scored) Rationale The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that the --read-only-port argument is set to 0. Remediation: By default, RKE2 starts the kubelet process with the --read-only-port argument set to 0.","title":"4.2.4"},{"location":"security/cis_self_assessment/#425","text":"Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Scored) Rationale Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify that there's nothing returned. Remediation: By default, RKE2 does not set --streaming-connection-idle-timeout when starting kubelet.","title":"4.2.5"},{"location":"security/cis_self_assessment/#426","text":"Ensure that the --protect-kernel-defaults argument is set to true (Scored) Rationale Kernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: When running with the profile flag set to cis-1.5 , RKE2 starts the kubelet process with the --protect-kernel-defaults argument set to true.","title":"4.2.6"},{"location":"security/cis_self_assessment/#427","text":"Ensure that the --make-iptables-util-chains argument is set to true (Scored) Rationale Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify there are no results returned. Remediation: By default, RKE2 does not set the --make-iptables-util-chains argument. No manual remediation needed.","title":"4.2.7"},{"location":"security/cis_self_assessment/#428","text":"Ensure that the --hostname-override argument is not set (Not Scored) Rationale Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Result: Not Applicable Remediation: RKE2 does set this parameter for each host, but RKE2 also manages all certificates in the cluster. It ensures the hostname-override is included as a subject alternative name (SAN) in the kubelet's certificate.","title":"4.2.8"},{"location":"security/cis_self_assessment/#429","text":"Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture (Not Scored) Rationale It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Result: Not Scored - Operator Dependent Remediation: See CIS Benchmark guide for further details on configuring this.","title":"4.2.9"},{"location":"security/cis_self_assessment/#4210","text":"Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Scored) Rationale Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic. Result: Pass Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Verify the --tls-cert-file and --tls-private-key-file arguments are present and set appropriately. Remediation: By default, RKE2 sets the --tls-cert-file and --tls-private-key-file arguments when executing the kubelet process.","title":"4.2.10"},{"location":"security/cis_self_assessment/#4211","text":"Ensure that the --rotate-certificates argument is not set to false (Scored) Rationale The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Note: This feature also require the RotateKubeletClientCertificate feature gate to be enabled (which is the default since Kubernetes v1.7) Result: Not Applicable Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation.","title":"4.2.11"},{"location":"security/cis_self_assessment/#4212","text":"Ensure that the RotateKubeletServerCertificate argument is set to true (Scored) Rationale RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Result: Not Applicable Audit: Run the below command on the master node. /bin/ps -ef | grep kubelet | grep -v grep Remediation: By default, RKE2 implements it's own logic for certificate generation and rotation.","title":"4.2.12"},{"location":"security/cis_self_assessment/#4213","text":"Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Not Scored) Rationale TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Result: Not Scored - Operator Dependent Remediation: Configuration of the parameter is dependent on your use case. Please see the CIS Kubernetes Benchmark for suggestions on configuring this for your usecase.","title":"4.2.13"},{"location":"security/cis_self_assessment/#5-kubernetes-policies","text":"","title":"5 Kubernetes Policies"},{"location":"security/cis_self_assessment/#51-rbac-and-service-accounts","text":"","title":"5.1 RBAC and Service Accounts"},{"location":"security/cis_self_assessment/#511","text":"Ensure that the cluster-admin role is only used where required (Not Scored) Rationale Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding , it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding , it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Result: Pass Remediation: RKE2 does not make inappropriate use of the cluster-admin role. Operators must audit their workloads of additional usage. See the CIS Benchmark guide for more details.","title":"5.1.1"},{"location":"security/cis_self_assessment/#512","text":"Minimize access to secrets (Not Scored) Rationale Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Result: Not Scored - Operator Dependent Remediation: RKE2 limits its use of secrets for the system components appropriately, but operators must audit the use of secrets by their workloads. See the CIS Benchmark guide for more details.","title":"5.1.2"},{"location":"security/cis_self_assessment/#513","text":"Minimize wildcard use in Roles and ClusterRoles (Not Scored) Rationale The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API. Result: Not Scored - Operator Dependent Audit: Run the below command on the master node. # Retrieve the roles defined across each namespaces in the cluster and review for wildcards /var/lib/rancher/rke2/bin/kubectl get roles --all-namespaces -o yaml # Retrieve the cluster roles defined in the cluster and review for wildcards /var/lib/rancher/rke2/bin/kubectl get clusterroles -o yaml Verify that there are not wildcards in use. Remediation: Operators should review their workloads for proper role usage. See the CIS Benchmark guide for more details.","title":"5.1.3"},{"location":"security/cis_self_assessment/#514","text":"Minimize access to create pods (Not Scored) Rationale The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Result: Not Scored - Operator Dependent Remediation: Operators should review who has access to create pods in their cluster. See the CIS Benchmark guide for more details.","title":"5.1.4"},{"location":"security/cis_self_assessment/#515","text":"Ensure that default service accounts are not actively used. (Scored) Rationale Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. Result: Pass. Currently requires operator intervention See the known issue for details. Audit: For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account. Remediation: Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false","title":"5.1.5"},{"location":"security/cis_self_assessment/#516","text":"Ensure that Service Account Tokens are only mounted where necessary (Not Scored) Rationale Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Result: Not Scored - Operator Dependent Remediation: The pods launched by RKE2 are part of the control plane and generally need access to communicate with the API server, thus this control does not apply to them. Operators should review their workloads and take steps to modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.","title":"5.1.6"},{"location":"security/cis_self_assessment/#52-pod-security-policies","text":"","title":"5.2 Pod Security Policies"},{"location":"security/cis_self_assessment/#521","text":"Minimize the admission of containers wishing to share the host process ID namespace (Scored) Rationale Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one PodSecurityPolicy (PSP) defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl describe psp global-restricted-psp | grep MustRunAsNonRoot Verify that the result is Rule: MustRunAsNonRoot . Remediation: RKE2, when run with the --profile=cis-1.5 argument, will disallow the use of privileged containers.","title":"5.2.1"},{"location":"security/cis_self_assessment/#522","text":"Minimize the admission of containers wishing to share the host process ID namespace (Scored) Rationale A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostPID == null) or (.spec.hostPID == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the hostPID value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed.","title":"5.2.2"},{"location":"security/cis_self_assessment/#523","text":"Minimize the admission of containers wishing to share the host IPC namespace (Scored) Rationale A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host IPC namespace. If you have a requirement to containers which require hostIPC, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostIPC == null) or (.spec.hostIPC == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the HostIPC value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed.","title":"5.2.3"},{"location":"security/cis_self_assessment/#524","text":"Minimize the admission of containers wishing to share the host network namespace (Scored) Rationale A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host network namespace. If you have need to run containers which require hostNetwork, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.hostNetwork == null) or (.spec.hostNetwork == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the HostNetwork value to false explicitly for the PSP it creates. When reviewing PSPs, note that the Kubernetes API only displays this field if it is explicitly set to true. No manual remediation is needed.","title":"5.2.4"},{"location":"security/cis_self_assessment/#525","text":"Minimize the admission of containers with allowPrivilegeEscalation (Scored) Rationale A container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent. There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.allowPrivilegeEscalation == null) or (.spec.allowPrivilegeEscalation == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the allowPrivilegeEscalation value to false explicitly for the PSP it creates. No manual remediation is needed.","title":"5.2.5"},{"location":"security/cis_self_assessment/#526","text":"Minimize the admission of root containers (Not Scored) Rationale Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user. There should be at least one PodSecurityPolicy (PSP) defined which does not permit root users in a container. If you need to run root containers, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp -o json | jq .items [] | jq -r 'select((.spec.allowPrivilegeEscalation == null) or (.spec.allowPrivilegeEscalation == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}' Verify that the returned count is 1. Remediation: RKE2 sets the runAsUser.Rule value to MustRunAsNonRoot in the PodSecurityPolicy that it creates. No manual remediation is needed.","title":"5.2.6"},{"location":"security/cis_self_assessment/#527","text":"Minimize the admission of containers with the NET_RAW capability (Not Scored) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability. There should be at least one PodSecurityPolicy (PSP) defined which prevents containers with the NET_RAW capability from launching. If you need to run containers with this capability, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp global-restricted-psp -o json | jq .spec.requiredDropCapabilities [] Verify the value is \"ALL\" . Remediation: RKE2 sets .spec.requiredDropCapabilities[] to a value of All . No manual remediation needed.","title":"5.2.7"},{"location":"security/cis_self_assessment/#528","text":"Minimize the admission of containers with added capabilities (Not Scored) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks. There should be at least one PodSecurityPolicy (PSP) defined which prevents containers with capabilities beyond the default set from launching. If you need to run containers with additional capabilities, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP. Result: Not Scored Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp Verify that there are no PSPs present which have allowedCapabilities set to anything other than an empty array. Remediation: When run with the --profile=cis-1.5 argument RKE2 applies a PodSecurityPolicy that sets requiredDropCapabilities to ALL . No manual remediation needed.","title":"5.2.8"},{"location":"security/cis_self_assessment/#529","text":"Minimize the admission of containers with capabilities assigned (Not Scored) Rationale Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized. Result: Not Scored Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get psp Remediation: When run with the --profile=cis-1.5 argument RKE2 applies a PodSecurityPolicy that sets requiredDropCapabilities to ALL . No manual remediation needed.","title":"5.2.9"},{"location":"security/cis_self_assessment/#53-network-policies-and-cni","text":"","title":"5.3 Network Policies and CNI"},{"location":"security/cis_self_assessment/#531","text":"Ensure that the CNI in use supports Network Policies (Not Scored) Rationale Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Result: Pass Audit: Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies. Remediation: By default, RKE2 use Canal (Calico and Flannel) and fully supports network policies.","title":"5.3.1"},{"location":"security/cis_self_assessment/#532","text":"Ensure that all Namespaces have Network Policies defined (Scored) Rationale Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Result: Pass Audit: Run the below command on the master node. for i in kube-system kube-public default ; do /var/lib/rancher/rke2/bin/kubectl get networkpolicies -n $i ; done Verify that there are network policies applied to each of the namespaces. Remediation: RKE2, when executed with the --profile=cis-1.5 argument applies a secure network policy that only allows intra-namespace traffic and DNS to kube-system. No manual remediation needed.","title":"5.3.2"},{"location":"security/cis_self_assessment/#54-secrets-management","text":"","title":"5.4 Secrets Management"},{"location":"security/cis_self_assessment/#541","text":"Prefer using secrets as files over secrets as environment variables (Not Scored) Rationale It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Result: Not Scored Audit: Run the following command to find references to objects which use environment variables defined from secrets. /var/lib/rancher/rke2/bin/kubectl get all -o jsonpath = '{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {\"\\n\"}{end}' -A Remediation: If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables.","title":"5.4.1"},{"location":"security/cis_self_assessment/#542","text":"Consider external secret storage (Not Scored) Rationale Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Result: Not Scored Audit: Review your secrets management implementation. Remediation: Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution.","title":"5.4.2"},{"location":"security/cis_self_assessment/#55-extensible-admission-control","text":"","title":"5.5 Extensible Admission Control"},{"location":"security/cis_self_assessment/#551","text":"Configure Image Provenance using ImagePolicyWebhook admission controller (Not Scored) Rationale Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster. Result: Not Scored Audit: Review the pod definitions in your cluster and verify that image provenance is configured as appropriate. Remediation: Follow the Kubernetes documentation and setup image provenance.","title":"5.5.1"},{"location":"security/cis_self_assessment/#56-omitted","text":"The v1.5.1 guide skips 5.6 and goes from 5.5 to 5.7. We are including it here merely for explanation.","title":"5.6 Omitted"},{"location":"security/cis_self_assessment/#57-general-policies","text":"These policies relate to general cluster management topics, like namespace best practices and policies applied to pod objects in the cluster.","title":"5.7 General Policies"},{"location":"security/cis_self_assessment/#571","text":"Create administrative boundaries between resources using namespaces (Not Scored) Rationale Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Result: Not Scored Audit: Run the below command and review the namespaces created in the cluster. /var/lib/rancher/rke2/bin/kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements. Remediation: Follow the documentation and create namespaces for objects in your deployment as you need them.","title":"5.7.1"},{"location":"security/cis_self_assessment/#572","text":"Ensure that the seccomp profile is set to docker/default in your pod definitions (Not Scored) Rationale Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container. Result: Not Scored Audit: Review the pod definitions in your cluster. It should create a line as below: annotations : seccomp.security.alpha.kubernetes.io/pod : docker/default Remediation: Review the Kubernetes documentation and if needed, apply a relevant PodSecurityPolicy.","title":"5.7.2"},{"location":"security/cis_self_assessment/#573","text":"Apply Security Context to Your Pods and Containers (Not Scored) Rationale A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Result: Not Scored Audit: Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate. Remediation: Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark.","title":"5.7.3"},{"location":"security/cis_self_assessment/#574","text":"The default namespace should not be used (Scored) Rationale Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Result: Pass Audit: Run the below command on the master node. /var/lib/rancher/rke2/bin/kubectl get all -n default Verify that there are no resources applied to the default namespace. Remediation: By default, RKE2 does not utilize the default namespace.","title":"5.7.4"},{"location":"security/fips_support/","text":"FIPS 140-2 is a U.S. Federal Government security standard used to approve cryptographic modules. This document explains how RKE2 is built with FIPS validated cryptographic libraries. Use of FIPS Compatible Go compiler. \u00b6 The Go compiler in use can be found here . Each component of the system is built with the version of this compiler that matches the same standard Go compiler version that would be used otherwise. This version of Go replaces the standard Go crypto libraries with the FIPS validated BoringCrypto module. See the readme for more details. Moreover, this module is currently being revalidated as the Rancher Kubernetes Cryptographic Library for the additional platforms and systems supported by RKE2. FIPS Support in Cluster Components \u00b6 Most of the components of the RKE2 system are statically compiled with the GoBoring Go compiler implementation that takes advantage of the BoringSSL library. RKE2, from a component perspective, is broken up in a number of sections. The list below contains the sections and associated components. Kubernetes API Server Controller Manager Scheduler Kubelet Kube Proxy Metric Server Kubectl Helm Charts Flannel Calico CoreDNS Runtime \u00b6 To ensure that all aspects of the system architecture are using FIPS 140-2 compliant algorithm implementations, the RKE2 runtime contains utilities statically compiled with the FIPS enabled Go compiler for FIPS 140-2 compliance. This ensures that all levels of the stack are compliant from Kubernetes daemons to container orchestration mechanics. etcd containerd containerd-shim containerd-shim-runc-v1 containerd-shim-runc-v2 ctr crictl runc Ingress \u00b6 The NGINX Ingress included with RKE2 is not currently FIPS enabled. It can, however, be disabled and replaced by the cluster operator/owner.","title":"FIPS 140-2 Enablement"},{"location":"security/fips_support/#use-of-fips-compatible-go-compiler","text":"The Go compiler in use can be found here . Each component of the system is built with the version of this compiler that matches the same standard Go compiler version that would be used otherwise. This version of Go replaces the standard Go crypto libraries with the FIPS validated BoringCrypto module. See the readme for more details. Moreover, this module is currently being revalidated as the Rancher Kubernetes Cryptographic Library for the additional platforms and systems supported by RKE2.","title":"Use of FIPS Compatible Go compiler."},{"location":"security/fips_support/#fips-support-in-cluster-components","text":"Most of the components of the RKE2 system are statically compiled with the GoBoring Go compiler implementation that takes advantage of the BoringSSL library. RKE2, from a component perspective, is broken up in a number of sections. The list below contains the sections and associated components. Kubernetes API Server Controller Manager Scheduler Kubelet Kube Proxy Metric Server Kubectl Helm Charts Flannel Calico CoreDNS","title":"FIPS Support in Cluster Components"},{"location":"security/fips_support/#runtime","text":"To ensure that all aspects of the system architecture are using FIPS 140-2 compliant algorithm implementations, the RKE2 runtime contains utilities statically compiled with the FIPS enabled Go compiler for FIPS 140-2 compliance. This ensures that all levels of the stack are compliant from Kubernetes daemons to container orchestration mechanics. etcd containerd containerd-shim containerd-shim-runc-v1 containerd-shim-runc-v2 ctr crictl runc","title":"Runtime"},{"location":"security/fips_support/#ingress","text":"The NGINX Ingress included with RKE2 is not currently FIPS enabled. It can, however, be disabled and replaced by the cluster operator/owner.","title":"Ingress"},{"location":"security/hardening_guide/","text":"CIS Hardening Guide \u00b6 This document provides prescriptive guidance for hardening a production installation of RKE2. It outlines the configurations and controls required to address Kubernetes benchmark controls from the Center for Information Security (CIS). For more detail about evaluating a hardened cluster against the official CIS benchmark, refer to the CIS Benchmark Rancher Self-Assessment Guide . RKE2 is designed to be \"hardened by default\" and pass the majority of the Kubernetes CIS controls without modification. There are a few notable exceptions to this that require manual intervention to fully pass the CIS Benchmark: RKE2 will not modify the host operating system. Therefore, you, the operator, must make a few Host-level modifications. Certain CIS policy controls for PodSecurityPolicies and NetworkPolicies will restrict the functionality of this cluster. You must opt into having RKE2 configuring these out of the box. To help ensure these above requirements are met, RKE2 can be started with the profile flag set to cis-1.5 . This flag generally does two things: Checks that host-level requirements have been met. If they haven't, RKE2 will exit with a fatal error describing the unmet requirements.. Configures runtime Pod Security Policies and Network Policies that allow the cluster to pass associated controls. Note: The profile's flag only valid value is cis-1.5 . It accepts a string value to allow for other profiles in the future. The following secction outlines the specific actions that are taken when the profile flag is set to cis-1.5 . Host-level Requirements \u00b6 There are two areas of of Host-level requirements: kernel parameters and etcd process/directory configuration. These are outlined in this section. Ensure protect-kernel-defaults is set \u00b6 This is a kubelet flag that will cause the kubelet to exit if the required kernel parameters are unset or are set to values that are different from the kubelet's defaults. When the profile flag is set, RKE2 will set the flag to true. Note: protect-kernel-defaults is exposed a top-level flag for RKE2. If you have set profile to \"cis-1.5\" and protect-kernel-defaults to false explicitly, RKE2 will exit with an error. RKE2 will also check the same kernel parameters that the kubelet does and exit with an error following the same rules as the kubelet. This is done as a convenience to help the operator more quickly and easily identify what kernel parameters are violationg the kubelet defaults. Ensure etcd is started properly \u00b6 The CIS Benchmark requires that the etcd data directory be owned by the etcd user and group. This implicitly requires the etcd process to be ran as the host-level etcd user. To acheive this, RKE2 takes several steps when started with the cis-1.5 profile: Check that the etcd user and group exists on the host. If they don't, exit with an error. Create etcd's data directory with etcd as the user and group owner. Ensure the etcd process is ran as the etcd user and group by setting the etcd static pod's SecurityContext approopriately. Setting up hosts \u00b6 This section gives you the commands necessary to configure your host to meet the above requirements. Set kernel parameters \u00b6 When RKE2 is installed, it creates a sysctl config file to set the required parameters appropriately. However, it does not automatically configure the Host to use this configuration. You must do this manually. The location of the config file depends on the installation method used. If RKE2 was installed via RPM, YUM, or DNF (the default on OSes that use RPMs, such as CentOS), run the following command(s): sudo cp -f /usr/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf sudo systemctl restart systemd-sysctl If RKE2 was installed via the tarball (the default on OSes that do not use RPMs, such as Ubuntu), run the following command: sudo cp -f /usr/local/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf sudo systemctl restart systemd-sysctl If your system lacks the systemd-sysctl.service and/or the /etc/sysctl.d directory you will want to make sure the sysctls are applied at boot by running the following command during start-up: sysctl -p /usr/local/share/rke2/rke2-cis-sysctl.conf Create the etcd user \u00b6 useradd -r -c \"etcd user\" -s /sbin/nologin -M etcd Kubernetes Runtime Requirements \u00b6 The runtime requirements to pass the CIS Benchmark are centered around pod security and network policies. These are outlined in this section. PodSecurityPolicies \u00b6 RKE2 always runs with the PodSecurityPolicy admission controller turned on. However, when it is not started with the cis-1.5 profile, RKE2 will put an unrestriced policy in place that allows Kubernetes to run as though the PodSecurityPolicy admission controller was not enabled. When ran with the cis-1.5 profile, RKE2 will put a much more restrictive set of policies in place. These policies meet the requirements outlined in section 5.2 of the CIS Benchmark. Note: The Kubernetes control plane components and critical additions such as CNI, DNS, and Ingress are ran as pods in the kube-system namespace. Therefore, this namespace will have a policy that is less restrictive so that these components can run properly. NetworkPolicies \u00b6 When ran with the cis-1.5 profile, RKE2 will put NetworkPolicies in place that pass the CIS Benchmark for Kubernetes's built-in namespcaes. These namespaces are: kube-system , kube-public , kube-node-lease , and default . The NetworkPolicy used will only allow pods within the same namespace to talk to each other. The notable exception to this is that it allows DNS requests to be resolved. Note: Operators must manage network policies as normal for additional namespaces that are created. Known Issues \u00b6 The following are controls that RKE2 currently does not pass. Each gap will be explained and whether it can be passed through manual operator intervention or if it will be addressed in a future release. Control 3.2.1 \u00b6 Ensure that a minimal audit policy is created (Scored) Rationale Logging is an important detective control for all systems, to detect potential unauthorised access. RKE2 currently does not support configuring audit logging. This is a known issue that will be addressed in an upcoming release. Control 5.1.5 \u00b6 Ensure that default service accounts are not actively used. (Scored) Rationale Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. The remediation for this is to update the automountServiceAccountToken field to false for the default service account in each namespace. For default service accounts in the built-in namespaces ( kube-system , kube-public , kube-node-lease , and default ), RKE2 does not automatically do this. You can manually update this field on these service accounts to passs the control. Conclusion \u00b6 If you have followed this guide, your RKE2 cluster will be configured to pass the CIS Kubernetes Benchmark. You can review our CIS Benchmark Self-Assessment Guide to understand how we verified each of the benchmark and how you can do the same on your cluster.","title":"CIS Hardening Guide"},{"location":"security/hardening_guide/#cis-hardening-guide","text":"This document provides prescriptive guidance for hardening a production installation of RKE2. It outlines the configurations and controls required to address Kubernetes benchmark controls from the Center for Information Security (CIS). For more detail about evaluating a hardened cluster against the official CIS benchmark, refer to the CIS Benchmark Rancher Self-Assessment Guide . RKE2 is designed to be \"hardened by default\" and pass the majority of the Kubernetes CIS controls without modification. There are a few notable exceptions to this that require manual intervention to fully pass the CIS Benchmark: RKE2 will not modify the host operating system. Therefore, you, the operator, must make a few Host-level modifications. Certain CIS policy controls for PodSecurityPolicies and NetworkPolicies will restrict the functionality of this cluster. You must opt into having RKE2 configuring these out of the box. To help ensure these above requirements are met, RKE2 can be started with the profile flag set to cis-1.5 . This flag generally does two things: Checks that host-level requirements have been met. If they haven't, RKE2 will exit with a fatal error describing the unmet requirements.. Configures runtime Pod Security Policies and Network Policies that allow the cluster to pass associated controls. Note: The profile's flag only valid value is cis-1.5 . It accepts a string value to allow for other profiles in the future. The following secction outlines the specific actions that are taken when the profile flag is set to cis-1.5 .","title":"CIS Hardening Guide"},{"location":"security/hardening_guide/#host-level-requirements","text":"There are two areas of of Host-level requirements: kernel parameters and etcd process/directory configuration. These are outlined in this section.","title":"Host-level Requirements"},{"location":"security/hardening_guide/#ensure-protect-kernel-defaults-is-set","text":"This is a kubelet flag that will cause the kubelet to exit if the required kernel parameters are unset or are set to values that are different from the kubelet's defaults. When the profile flag is set, RKE2 will set the flag to true. Note: protect-kernel-defaults is exposed a top-level flag for RKE2. If you have set profile to \"cis-1.5\" and protect-kernel-defaults to false explicitly, RKE2 will exit with an error. RKE2 will also check the same kernel parameters that the kubelet does and exit with an error following the same rules as the kubelet. This is done as a convenience to help the operator more quickly and easily identify what kernel parameters are violationg the kubelet defaults.","title":"Ensure protect-kernel-defaults is set"},{"location":"security/hardening_guide/#ensure-etcd-is-started-properly","text":"The CIS Benchmark requires that the etcd data directory be owned by the etcd user and group. This implicitly requires the etcd process to be ran as the host-level etcd user. To acheive this, RKE2 takes several steps when started with the cis-1.5 profile: Check that the etcd user and group exists on the host. If they don't, exit with an error. Create etcd's data directory with etcd as the user and group owner. Ensure the etcd process is ran as the etcd user and group by setting the etcd static pod's SecurityContext approopriately.","title":"Ensure etcd is started properly"},{"location":"security/hardening_guide/#setting-up-hosts","text":"This section gives you the commands necessary to configure your host to meet the above requirements.","title":"Setting up hosts"},{"location":"security/hardening_guide/#set-kernel-parameters","text":"When RKE2 is installed, it creates a sysctl config file to set the required parameters appropriately. However, it does not automatically configure the Host to use this configuration. You must do this manually. The location of the config file depends on the installation method used. If RKE2 was installed via RPM, YUM, or DNF (the default on OSes that use RPMs, such as CentOS), run the following command(s): sudo cp -f /usr/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf sudo systemctl restart systemd-sysctl If RKE2 was installed via the tarball (the default on OSes that do not use RPMs, such as Ubuntu), run the following command: sudo cp -f /usr/local/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf sudo systemctl restart systemd-sysctl If your system lacks the systemd-sysctl.service and/or the /etc/sysctl.d directory you will want to make sure the sysctls are applied at boot by running the following command during start-up: sysctl -p /usr/local/share/rke2/rke2-cis-sysctl.conf","title":"Set kernel parameters"},{"location":"security/hardening_guide/#create-the-etcd-user","text":"useradd -r -c \"etcd user\" -s /sbin/nologin -M etcd","title":"Create the etcd user"},{"location":"security/hardening_guide/#kubernetes-runtime-requirements","text":"The runtime requirements to pass the CIS Benchmark are centered around pod security and network policies. These are outlined in this section.","title":"Kubernetes Runtime Requirements"},{"location":"security/hardening_guide/#podsecuritypolicies","text":"RKE2 always runs with the PodSecurityPolicy admission controller turned on. However, when it is not started with the cis-1.5 profile, RKE2 will put an unrestriced policy in place that allows Kubernetes to run as though the PodSecurityPolicy admission controller was not enabled. When ran with the cis-1.5 profile, RKE2 will put a much more restrictive set of policies in place. These policies meet the requirements outlined in section 5.2 of the CIS Benchmark. Note: The Kubernetes control plane components and critical additions such as CNI, DNS, and Ingress are ran as pods in the kube-system namespace. Therefore, this namespace will have a policy that is less restrictive so that these components can run properly.","title":"PodSecurityPolicies"},{"location":"security/hardening_guide/#networkpolicies","text":"When ran with the cis-1.5 profile, RKE2 will put NetworkPolicies in place that pass the CIS Benchmark for Kubernetes's built-in namespcaes. These namespaces are: kube-system , kube-public , kube-node-lease , and default . The NetworkPolicy used will only allow pods within the same namespace to talk to each other. The notable exception to this is that it allows DNS requests to be resolved. Note: Operators must manage network policies as normal for additional namespaces that are created.","title":"NetworkPolicies"},{"location":"security/hardening_guide/#known-issues","text":"The following are controls that RKE2 currently does not pass. Each gap will be explained and whether it can be passed through manual operator intervention or if it will be addressed in a future release.","title":"Known Issues"},{"location":"security/hardening_guide/#control-321","text":"Ensure that a minimal audit policy is created (Scored) Rationale Logging is an important detective control for all systems, to detect potential unauthorised access. RKE2 currently does not support configuring audit logging. This is a known issue that will be addressed in an upcoming release.","title":"Control 3.2.1"},{"location":"security/hardening_guide/#control-515","text":"Ensure that default service accounts are not actively used. (Scored) Rationale Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. The remediation for this is to update the automountServiceAccountToken field to false for the default service account in each namespace. For default service accounts in the built-in namespaces ( kube-system , kube-public , kube-node-lease , and default ), RKE2 does not automatically do this. You can manually update this field on these service accounts to passs the control.","title":"Control 5.1.5"},{"location":"security/hardening_guide/#conclusion","text":"If you have followed this guide, your RKE2 cluster will be configured to pass the CIS Kubernetes Benchmark. You can review our CIS Benchmark Self-Assessment Guide to understand how we verified each of the benchmark and how you can do the same on your cluster.","title":"Conclusion"},{"location":"security/policies/","text":"This document describes how RKE2 configures PodSecurityPolicies and NetworkPolicies in order to be secure-by-default while also providing operators with maximum configuration flexibility. Pod Security Policies \u00b6 RKE2 can be ran with or without the --profile=cis-1.5 argument. This will cause it to apply different PodSecurityPolicies (PSPs) at start-up. If ran with the cis-1.5 profile, RKE2 will apply a restrictive policy called global-restricted-psp to all namespaces except kube-system . The kube-system namespace needs a less restrictive policy named system-unrestricted-psp in order to launch critical components. If ran without the cis-1.5 profile, RKE2 will apply a completely unrestricted policy called global-unrestricted-psp , which is the equivalent of running without the PSP admission controller enabled. RKE2 will put these policies in place upon initial startup, but will not modify them after that, unless explicitly triggered by the cluster operator as described below. This is to allow the operator to fully control the PSPs without RKE2's defaults adding interference. Creation and application of the PSPs is controlled by the presence or absence of certain annotations on the kube-system namespace. These map directly to the PSPs which can be created and are: psp.rke2.io/global-restricted psp.rke2.io/system-unrestricted psp.rke2.io/global-unrestricted The following logic is performed at startup for the policies and their annotations: If the annotation exists, RKE2 continues without further action. If the annotation doesn't exist, RKE2 checks to see if the associated policy exists and if so, deletes and recreates it, along with adding the annotation to the namespace. In the case of the global-unrestricted-psp , the policy is not recreated. This is to account for moving between CIS and non-CIS modes without making the cluster less secure. At the time of creating a policy, cluster roles and cluster role bindings are also created to ensure the appropriate policies are put into use by default. So, after initial start-up, operators can modify or delete RKE2's policies and RKE2 will respect those changes. Additionally, to \"reset\" a policy, an operator just needs to delete the associated annotation from the kube-system namespace and restart RKE2. The policies are outlined below. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : global-restricted-psp annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : 'docker/default,runtime/default' apparmor.security.beta.kubernetes.io/allowedProfileNames : 'runtime/default' seccomp.security.alpha.kubernetes.io/defaultProfileName : 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName : 'runtime/default' spec : privileged : false # CIS - 5.2.1 allowPrivilegeEscalation : false # CIS - 5.2.5 requiredDropCapabilities : # CIS - 5.2.7/8/9 - ALL volumes : - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' - 'persistentVolumeClaim' hostNetwork : false # CIS - 5.2.4 hostIPC : false # CIS - 5.2.3 hostPID : false # CIS - 5.2.2 runAsUser : rule : 'MustRunAsNonRoot' # CIS - 5.2.6 seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'MustRunAs' ranges : - min : 1 max : 65535 fsGroup : rule : 'MustRunAs' ranges : - min : 1 max : 65535 readOnlyRootFilesystem : false If RKE2 is started in non CIS mode, annotations are checked like above however the resulting application of pod security policies is a permissive one. See below. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : global-unrestricted-psp annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : - '*' volumes : - '*' hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' In both cases, the \"system unrestricted policy\" is applied. See below. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : system-unrestricted-psp annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : - '*' volumes : - '*' hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' To view the podSecurityPolicies currently deployed on your system, run the below command: kubectl get psp -A Network Policies \u00b6 When RKE2 is run with the --profile=cis-1.5 argument, it will apply 2 network policies to the kube-system , kube-public , and default namespaces and applies associated annotations. The same logic applies to these policies and annotations as the PSPs. On start, the annotations for each namespace are checked for existence and if they exist, RKE2 takes no action. If the annotation doesn't exist, RKE2 checks to see if the policy exists and if it does, recreates it. The first policy applied is to restrict network traffic to only the namespace itself. See below. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : managedFields : - apiVersion : networking.k8s.io/v1 fieldsType : FieldsV1 fieldsV1 : f:spec : f:ingress : {} f:policyTypes : {} name : default-network-policy namespace : default spec : ingress : - from : - podSelector : {} podSelector : {} policyTypes : - Ingress The second policy applied is to the kube-system namespace and allows for DNS traffic. See below. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : managedFields : - apiVersion : networking.k8s.io/v1 fieldsV1 : f:spec : f:ingress : {} f:podSelector : f:matchLabels : f:policyTypes : {} name : default-network-dns-policy namespace : kube-system spec : ingress : - ports : - port : 53 protocol : TCP - port : 53 protocol : UDP podSelector : matchLabels : policyTypes : - Ingress RKE2 applies the default-network-policy policy and np.rke2.io annotation to all built-in namespaces. The kube-system namespace additionally gets the default-network-dns-policy policy and np.rke2.io/dns annotation applied to it. To view the network policies currently deployed on your system, run the below command: kubectl get networkpolicies -A","title":"Default Policy Configuration"},{"location":"security/policies/#pod-security-policies","text":"RKE2 can be ran with or without the --profile=cis-1.5 argument. This will cause it to apply different PodSecurityPolicies (PSPs) at start-up. If ran with the cis-1.5 profile, RKE2 will apply a restrictive policy called global-restricted-psp to all namespaces except kube-system . The kube-system namespace needs a less restrictive policy named system-unrestricted-psp in order to launch critical components. If ran without the cis-1.5 profile, RKE2 will apply a completely unrestricted policy called global-unrestricted-psp , which is the equivalent of running without the PSP admission controller enabled. RKE2 will put these policies in place upon initial startup, but will not modify them after that, unless explicitly triggered by the cluster operator as described below. This is to allow the operator to fully control the PSPs without RKE2's defaults adding interference. Creation and application of the PSPs is controlled by the presence or absence of certain annotations on the kube-system namespace. These map directly to the PSPs which can be created and are: psp.rke2.io/global-restricted psp.rke2.io/system-unrestricted psp.rke2.io/global-unrestricted The following logic is performed at startup for the policies and their annotations: If the annotation exists, RKE2 continues without further action. If the annotation doesn't exist, RKE2 checks to see if the associated policy exists and if so, deletes and recreates it, along with adding the annotation to the namespace. In the case of the global-unrestricted-psp , the policy is not recreated. This is to account for moving between CIS and non-CIS modes without making the cluster less secure. At the time of creating a policy, cluster roles and cluster role bindings are also created to ensure the appropriate policies are put into use by default. So, after initial start-up, operators can modify or delete RKE2's policies and RKE2 will respect those changes. Additionally, to \"reset\" a policy, an operator just needs to delete the associated annotation from the kube-system namespace and restart RKE2. The policies are outlined below. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : global-restricted-psp annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : 'docker/default,runtime/default' apparmor.security.beta.kubernetes.io/allowedProfileNames : 'runtime/default' seccomp.security.alpha.kubernetes.io/defaultProfileName : 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName : 'runtime/default' spec : privileged : false # CIS - 5.2.1 allowPrivilegeEscalation : false # CIS - 5.2.5 requiredDropCapabilities : # CIS - 5.2.7/8/9 - ALL volumes : - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' - 'persistentVolumeClaim' hostNetwork : false # CIS - 5.2.4 hostIPC : false # CIS - 5.2.3 hostPID : false # CIS - 5.2.2 runAsUser : rule : 'MustRunAsNonRoot' # CIS - 5.2.6 seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'MustRunAs' ranges : - min : 1 max : 65535 fsGroup : rule : 'MustRunAs' ranges : - min : 1 max : 65535 readOnlyRootFilesystem : false If RKE2 is started in non CIS mode, annotations are checked like above however the resulting application of pod security policies is a permissive one. See below. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : global-unrestricted-psp annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : - '*' volumes : - '*' hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' In both cases, the \"system unrestricted policy\" is applied. See below. apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : system-unrestricted-psp annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : - '*' volumes : - '*' hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' To view the podSecurityPolicies currently deployed on your system, run the below command: kubectl get psp -A","title":"Pod Security Policies"},{"location":"security/policies/#network-policies","text":"When RKE2 is run with the --profile=cis-1.5 argument, it will apply 2 network policies to the kube-system , kube-public , and default namespaces and applies associated annotations. The same logic applies to these policies and annotations as the PSPs. On start, the annotations for each namespace are checked for existence and if they exist, RKE2 takes no action. If the annotation doesn't exist, RKE2 checks to see if the policy exists and if it does, recreates it. The first policy applied is to restrict network traffic to only the namespace itself. See below. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : managedFields : - apiVersion : networking.k8s.io/v1 fieldsType : FieldsV1 fieldsV1 : f:spec : f:ingress : {} f:policyTypes : {} name : default-network-policy namespace : default spec : ingress : - from : - podSelector : {} podSelector : {} policyTypes : - Ingress The second policy applied is to the kube-system namespace and allows for DNS traffic. See below. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : managedFields : - apiVersion : networking.k8s.io/v1 fieldsV1 : f:spec : f:ingress : {} f:podSelector : f:matchLabels : f:policyTypes : {} name : default-network-dns-policy namespace : kube-system spec : ingress : - ports : - port : 53 protocol : TCP - port : 53 protocol : UDP podSelector : matchLabels : policyTypes : - Ingress RKE2 applies the default-network-policy policy and np.rke2.io annotation to all built-in namespaces. The kube-system namespace additionally gets the default-network-dns-policy policy and np.rke2.io/dns annotation applied to it. To view the network policies currently deployed on your system, run the below command: kubectl get networkpolicies -A","title":"Network Policies"}]}